{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "SefOcHiQBp0K"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2J8KWSfjB0AE",
    "outputId": "1401dbf7-656d-40c4-dbbe-39ff86d8ce14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Nov  9 21:15:52 2020       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 410.79       Driver Version: 410.79       CUDA Version: 10.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla K80           Off  | 00000000:3D:00.0 Off |                    0 |\n",
      "| N/A   41C    P0    53W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla K80           Off  | 00000000:3E:00.0 Off |                    0 |\n",
      "| N/A   39C    P0    81W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla K80           Off  | 00000000:88:00.0 Off |                    0 |\n",
      "| N/A   38C    P0    64W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla K80           Off  | 00000000:89:00.0 Off |                    0 |\n",
      "| N/A   33C    P0    74W / 149W |      0MiB / 11441MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "2aAKlLSqBp0O"
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels, channels, stride):\n",
    "        \n",
    "        super(BasicBlock, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels,\n",
    "                               out_channels=channels,\n",
    "                               kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=channels,\n",
    "                             out_channels=channels,\n",
    "                             kernel_size=3, stride=1,\n",
    "                             padding=1, bias=False)\n",
    "        \n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        \n",
    "        if stride != 1 or in_channels != channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=in_channels,\n",
    "                          out_channels=channels,\n",
    "                          kernel_size=1, stride=stride,\n",
    "                          padding=0, bias=False),\n",
    "                nn.BatchNorm2d(channels)\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "OJwK48cZBp0R"
   },
   "outputs": [],
   "source": [
    "class BottleNeck(nn.Module):\n",
    "    \n",
    "    expansion = 4\n",
    "    \n",
    "    def __init__(self, in_channels, channels, stride):\n",
    "        \n",
    "        super(BottleNeck, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels,\n",
    "                               out_channels=channels,\n",
    "                               kernel_size=1, stride=1,\n",
    "                               padding=0, bias=False)\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=channels,\n",
    "                               out_channels=channels,\n",
    "                               kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        \n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=channels,\n",
    "                               out_channels=self.expansion * channels,\n",
    "                               kernel_size=1, stride=1,\n",
    "                               padding=0, bias=False)\n",
    "        \n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion * channels)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        \n",
    "        if stride != 1 or in_channels != self.expansion * channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=in_channels,\n",
    "                          out_channels=self.expansion * channels,\n",
    "                          kernel_size=1, stride=stride,\n",
    "                          padding=0, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion * channels)\n",
    "            )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "S0leaV7FBp0V"
   },
   "outputs": [],
   "source": [
    "class ResNet18(nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(ResNet18, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels=3, out_channels=64,\n",
    "                              kernel_size=7, stride=2,\n",
    "                              padding=3)\n",
    "        \n",
    "        self.bn = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.maxpool2d = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        layers.append(BasicBlock(64, 64, 1))\n",
    "        layers.append(BasicBlock(64, 64, 1))\n",
    "        \n",
    "        layers.append(BasicBlock(64, 128, 2))\n",
    "        layers.append(BasicBlock(128, 128, 1))\n",
    "        \n",
    "        layers.append(BasicBlock(128, 256, 2))\n",
    "        layers.append(BasicBlock(256, 256, 1))\n",
    "        \n",
    "        layers.append(BasicBlock(256, 512, 2))\n",
    "        layers.append(BasicBlock(512, 512, 1))\n",
    "        \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "        self.avgpool2d = nn.AvgPool2d(kernel_size=7, stride=1)\n",
    "        self.fc = nn.Linear(512, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn(self.conv(x)))\n",
    "        \n",
    "        out = self.maxpool2d(out)\n",
    "        \n",
    "        out = self.layers(out)\n",
    "        \n",
    "        out = self.avgpool2d(out)\n",
    "        \n",
    "        out = out.view(out.size(0), -1)\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fztvLonPBp0Y"
   },
   "outputs": [],
   "source": [
    "class ResNet50(nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(ResNet50, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Conv2d(in_channels=3, out_channels=64,\n",
    "                              kernel_size=7, stride=2,\n",
    "                              padding=3)\n",
    "        \n",
    "        self.bn = nn.BatchNorm2d(64)\n",
    "        \n",
    "        self.maxpool2d = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        layers = []\n",
    "        \n",
    "        layers.append(BottleNeck(64, 64, 1)) # 这个block只是增加了图片的通道数,并没有改变图片的大小\n",
    "        layers.append(BottleNeck(256, 64, 1))\n",
    "        layers.append(BottleNeck(256, 64, 1))\n",
    "        \n",
    "        layers.append(BottleNeck(256, 128, 2)) # 这个block增加了图片的通道数,也改变了图片的大小\n",
    "        layers.append(BottleNeck(512, 128, 1))\n",
    "        layers.append(BottleNeck(512, 128, 1))\n",
    "        layers.append(BottleNeck(512, 128, 1))\n",
    "        \n",
    "        layers.append(BottleNeck(512, 256, 2)) # 这个block增加了图片的通道数,也改变了图片的大小\n",
    "        layers.append(BottleNeck(1024, 256, 1))\n",
    "        layers.append(BottleNeck(1024, 256, 1))\n",
    "        layers.append(BottleNeck(1024, 256, 1))\n",
    "        layers.append(BottleNeck(1024, 256, 1))\n",
    "        layers.append(BottleNeck(1024, 256, 1))\n",
    "        \n",
    "        layers.append(BottleNeck(1024, 512, 2)) # 这个block增加了图片的通道数,也改变了图片的大小\n",
    "        layers.append(BottleNeck(2048, 512, 1))\n",
    "        layers.append(BottleNeck(2048, 512, 1))\n",
    "        \n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "        self.avgpool2d = nn.AvgPool2d(kernel_size=7, stride=1)\n",
    "        self.fc = nn.Linear(2048, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn(self.conv(x)))\n",
    "        \n",
    "        out = self.maxpool2d(out)\n",
    "        \n",
    "        out = self.layers(out)\n",
    "        \n",
    "        out = self.avgpool2d(out)\n",
    "        \n",
    "        out = out.view(out.size(0), -1)\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "GKGV2g7oBp0b"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.Resize(224),  # 长宽比不变，保持最短边为224\n",
    "     transforms.CenterCrop(224),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "dRaMyV40Bp0e"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCH = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZXszX16qBp0h",
    "outputId": "ea37d57b-efb9-4b46-d9ca-3f66b9f4c271"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_set = datasets.CIFAR10(root='./data/', train=True,\n",
    "                            download=True, transform=transform)\n",
    "\n",
    "DATASET_SIZE = len(train_set.data)\n",
    "\n",
    "train_set, valid_set = torch.utils.data.random_split(train_set, [int(DATASET_SIZE * 0.8), int(DATASET_SIZE * 0.2)])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True)\n",
    " \n",
    "valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True)\n",
    "\n",
    "test_set = datasets.CIFAR10(root='./data/', train=False,\n",
    "                           download=True, transform=transform)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE,\n",
    "                                         shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "n5AbmRmhBp0k"
   },
   "outputs": [],
   "source": [
    "def train(\n",
    "    model,\n",
    "    data_loader, \n",
    "    optimizer,\n",
    "    criterion,\n",
    "    print_every=30\n",
    "    ):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    print_loss_total = 0\n",
    "    epoch_loss = 0\n",
    "    for index, (images, labels) in enumerate(data_loader):\n",
    "        \n",
    "        images = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(images)\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        print_loss_total += loss.item()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if print_every and (index + 1) % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('\\tCurrent Loss: %.4f' % print_loss_avg)\n",
    "\n",
    "    return epoch_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "w1GA9s-xBp0n"
   },
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    model,\n",
    "    data_loader,\n",
    "    ):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        valid_correct = 0\n",
    "        for images, labels in data_loader:\n",
    "\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            outputs = model(images)\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            valid_correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return 100.0 * valid_correct / len(data_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "OaKh7uZJBp0r"
   },
   "outputs": [],
   "source": [
    "def test(\n",
    "    model,\n",
    "    data_loader,\n",
    "    ):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_correct = 0\n",
    "        for images, labels in data_loader:\n",
    "\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            outputs = model(images)\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(\"The accuracy of total {} images: {}%\".format(len(data_loader.dataset),\n",
    "                                                                  100.0 * test_correct / len(data_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "aO2Hv9MmBp0u"
   },
   "outputs": [],
   "source": [
    "official_resnet50 = models.resnet50(pretrained=False)\n",
    "official_resnet18 = models.resnet18(pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "qKZeXWKgBp0w"
   },
   "outputs": [],
   "source": [
    "official_resnet50.fc = nn.Linear(2048, 10)\n",
    "official_resnet18.fc = nn.Linear(512, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "uwdONRsYBp00"
   },
   "outputs": [],
   "source": [
    "custom_resnet50 = ResNet50()\n",
    "custom_resnet18 = ResNet18()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "BnwEXtoyBp08"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "9qx8YSZeBp03",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "official_resnet18.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 854
    },
    "id": "0odoc0L5Bp0_",
    "outputId": "8799355c-cc44-4bf5-d72b-8749fcbc8cf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tCurrent Loss: 2.1720\n",
      "\tCurrent Loss: 1.8507\n",
      "\tCurrent Loss: 1.7559\n",
      "\tCurrent Loss: 1.7065\n",
      "\tCurrent Loss: 1.6064\n",
      "\tCurrent Loss: 1.5855\n",
      "\tCurrent Loss: 1.5887\n",
      "\tCurrent Loss: 1.4584\n",
      "\tCurrent Loss: 1.5043\n",
      "\tCurrent Loss: 1.4553\n",
      "\tCurrent Loss: 1.3496\n",
      "\tCurrent Loss: 1.3513\n",
      "\tCurrent Loss: 1.3049\n",
      "\tCurrent Loss: 1.2444\n",
      "\tCurrent Loss: 1.2438\n",
      "\tCurrent Loss: 1.3089\n",
      "\tCurrent Loss: 1.2326\n",
      "\tCurrent Loss: 1.0851\n",
      "\tCurrent Loss: 1.1696\n",
      "\tCurrent Loss: 1.1600\n",
      "Train Epoch 0 Train Loss: 1.4424649998664856 Valid Acc: 54.78%\n",
      "\tCurrent Loss: 1.0814\n",
      "\tCurrent Loss: 1.0491\n",
      "\tCurrent Loss: 1.0384\n",
      "\tCurrent Loss: 0.9840\n",
      "\tCurrent Loss: 1.0357\n",
      "\tCurrent Loss: 1.0321\n",
      "\tCurrent Loss: 0.9522\n",
      "\tCurrent Loss: 0.9273\n",
      "\tCurrent Loss: 0.9549\n",
      "\tCurrent Loss: 0.9623\n",
      "\tCurrent Loss: 0.8836\n",
      "\tCurrent Loss: 0.9325\n",
      "\tCurrent Loss: 0.9071\n",
      "\tCurrent Loss: 0.8672\n",
      "\tCurrent Loss: 0.8506\n",
      "\tCurrent Loss: 0.7921\n",
      "\tCurrent Loss: 0.8637\n",
      "\tCurrent Loss: 0.8447\n",
      "\tCurrent Loss: 0.8322\n",
      "\tCurrent Loss: 0.8088\n",
      "Train Epoch 1 Train Loss: 0.9224644768714905 Valid Acc: 66.23%\n",
      "\tCurrent Loss: 0.7423\n",
      "\tCurrent Loss: 0.7630\n",
      "\tCurrent Loss: 0.7449\n",
      "\tCurrent Loss: 0.6868\n",
      "\tCurrent Loss: 0.7267\n",
      "\tCurrent Loss: 0.6988\n",
      "\tCurrent Loss: 0.6585\n",
      "\tCurrent Loss: 0.7444\n",
      "\tCurrent Loss: 0.6865\n",
      "\tCurrent Loss: 0.6851\n",
      "\tCurrent Loss: 0.6844\n",
      "\tCurrent Loss: 0.6998\n",
      "\tCurrent Loss: 0.6655\n",
      "\tCurrent Loss: 0.6939\n",
      "\tCurrent Loss: 0.6681\n",
      "\tCurrent Loss: 0.6496\n",
      "\tCurrent Loss: 0.6382\n",
      "\tCurrent Loss: 0.6702\n",
      "\tCurrent Loss: 0.6285\n",
      "\tCurrent Loss: 0.6439\n",
      "Train Epoch 2 Train Loss: 0.6873947854042053 Valid Acc: 72.46%\n",
      "\tCurrent Loss: 0.5506\n",
      "\tCurrent Loss: 0.5436\n",
      "\tCurrent Loss: 0.5472\n",
      "\tCurrent Loss: 0.5178\n",
      "\tCurrent Loss: 0.5736\n",
      "\tCurrent Loss: 0.5333\n",
      "\tCurrent Loss: 0.5514\n",
      "\tCurrent Loss: 0.5497\n",
      "\tCurrent Loss: 0.5195\n",
      "\tCurrent Loss: 0.5138\n",
      "\tCurrent Loss: 0.5574\n",
      "\tCurrent Loss: 0.5393\n",
      "\tCurrent Loss: 0.6150\n",
      "\tCurrent Loss: 0.5238\n",
      "\tCurrent Loss: 0.5454\n",
      "\tCurrent Loss: 0.5318\n",
      "\tCurrent Loss: 0.5697\n",
      "\tCurrent Loss: 0.5308\n",
      "\tCurrent Loss: 0.5477\n",
      "\tCurrent Loss: 0.5334\n",
      "Train Epoch 3 Train Loss: 0.5440321800708771 Valid Acc: 77.44%\n",
      "\tCurrent Loss: 0.4206\n",
      "\tCurrent Loss: 0.4009\n",
      "\tCurrent Loss: 0.4296\n",
      "\tCurrent Loss: 0.4310\n",
      "\tCurrent Loss: 0.4264\n",
      "\tCurrent Loss: 0.4330\n",
      "\tCurrent Loss: 0.4244\n",
      "\tCurrent Loss: 0.4370\n",
      "\tCurrent Loss: 0.4541\n",
      "\tCurrent Loss: 0.4595\n",
      "\tCurrent Loss: 0.4679\n",
      "\tCurrent Loss: 0.4334\n",
      "\tCurrent Loss: 0.4543\n",
      "\tCurrent Loss: 0.4338\n",
      "\tCurrent Loss: 0.4140\n",
      "\tCurrent Loss: 0.4101\n",
      "\tCurrent Loss: 0.4535\n",
      "\tCurrent Loss: 0.4444\n",
      "\tCurrent Loss: 0.3984\n",
      "\tCurrent Loss: 0.4310\n",
      "Train Epoch 4 Train Loss: 0.4328284314155579 Valid Acc: 77.75%\n",
      "\tCurrent Loss: 0.3396\n",
      "\tCurrent Loss: 0.3229\n",
      "\tCurrent Loss: 0.3419\n",
      "\tCurrent Loss: 0.3297\n",
      "\tCurrent Loss: 0.2887\n",
      "\tCurrent Loss: 0.3482\n",
      "\tCurrent Loss: 0.3765\n",
      "\tCurrent Loss: 0.3626\n",
      "\tCurrent Loss: 0.3003\n",
      "\tCurrent Loss: 0.3363\n",
      "\tCurrent Loss: 0.3200\n",
      "\tCurrent Loss: 0.3640\n",
      "\tCurrent Loss: 0.3146\n",
      "\tCurrent Loss: 0.3390\n",
      "\tCurrent Loss: 0.3929\n",
      "\tCurrent Loss: 0.3471\n",
      "\tCurrent Loss: 0.3643\n",
      "\tCurrent Loss: 0.3732\n",
      "\tCurrent Loss: 0.3592\n",
      "\tCurrent Loss: 0.3777\n",
      "Train Epoch 5 Train Loss: 0.34663616089820865 Valid Acc: 81.65%\n",
      "\tCurrent Loss: 0.2306\n",
      "\tCurrent Loss: 0.2135\n",
      "\tCurrent Loss: 0.2216\n",
      "\tCurrent Loss: 0.2224\n",
      "\tCurrent Loss: 0.2192\n",
      "\tCurrent Loss: 0.2601\n",
      "\tCurrent Loss: 0.2338\n",
      "\tCurrent Loss: 0.2697\n",
      "\tCurrent Loss: 0.2729\n",
      "\tCurrent Loss: 0.2778\n",
      "\tCurrent Loss: 0.2494\n",
      "\tCurrent Loss: 0.2540\n",
      "\tCurrent Loss: 0.2922\n",
      "\tCurrent Loss: 0.2687\n",
      "\tCurrent Loss: 0.2897\n",
      "\tCurrent Loss: 0.2881\n",
      "\tCurrent Loss: 0.2736\n",
      "\tCurrent Loss: 0.2831\n",
      "\tCurrent Loss: 0.2837\n",
      "\tCurrent Loss: 0.3115\n",
      "Train Epoch 6 Train Loss: 0.2607278817772865 Valid Acc: 79.82%\n",
      "\tCurrent Loss: 0.1651\n",
      "\tCurrent Loss: 0.1436\n",
      "\tCurrent Loss: 0.1489\n",
      "\tCurrent Loss: 0.1535\n",
      "\tCurrent Loss: 0.1734\n",
      "\tCurrent Loss: 0.1921\n",
      "\tCurrent Loss: 0.1929\n",
      "\tCurrent Loss: 0.1874\n",
      "\tCurrent Loss: 0.1939\n",
      "\tCurrent Loss: 0.1895\n",
      "\tCurrent Loss: 0.1794\n",
      "\tCurrent Loss: 0.1868\n",
      "\tCurrent Loss: 0.2018\n",
      "\tCurrent Loss: 0.2007\n",
      "\tCurrent Loss: 0.2256\n",
      "\tCurrent Loss: 0.1894\n",
      "\tCurrent Loss: 0.1872\n",
      "\tCurrent Loss: 0.2023\n",
      "\tCurrent Loss: 0.1913\n",
      "\tCurrent Loss: 0.2305\n",
      "Train Epoch 7 Train Loss: 0.18802600219249727 Valid Acc: 81.57%\n",
      "\tCurrent Loss: 0.1312\n",
      "\tCurrent Loss: 0.1207\n",
      "\tCurrent Loss: 0.0913\n",
      "\tCurrent Loss: 0.0931\n",
      "\tCurrent Loss: 0.0942\n",
      "\tCurrent Loss: 0.1147\n",
      "\tCurrent Loss: 0.1337\n",
      "\tCurrent Loss: 0.1089\n",
      "\tCurrent Loss: 0.1267\n",
      "\tCurrent Loss: 0.1268\n",
      "\tCurrent Loss: 0.1217\n",
      "\tCurrent Loss: 0.1385\n",
      "\tCurrent Loss: 0.1491\n",
      "\tCurrent Loss: 0.1629\n",
      "\tCurrent Loss: 0.1482\n",
      "\tCurrent Loss: 0.1662\n",
      "\tCurrent Loss: 0.1617\n",
      "\tCurrent Loss: 0.1444\n",
      "\tCurrent Loss: 0.1548\n",
      "\tCurrent Loss: 0.1571\n",
      "Train Epoch 8 Train Loss: 0.13299707710444927 Valid Acc: 80.75%\n",
      "\tCurrent Loss: 0.0971\n",
      "\tCurrent Loss: 0.0907\n",
      "\tCurrent Loss: 0.0842\n",
      "\tCurrent Loss: 0.0785\n",
      "\tCurrent Loss: 0.0967\n",
      "\tCurrent Loss: 0.1087\n",
      "\tCurrent Loss: 0.1017\n",
      "\tCurrent Loss: 0.0909\n",
      "\tCurrent Loss: 0.0932\n",
      "\tCurrent Loss: 0.0829\n",
      "\tCurrent Loss: 0.0879\n",
      "\tCurrent Loss: 0.1052\n",
      "\tCurrent Loss: 0.0981\n",
      "\tCurrent Loss: 0.1049\n",
      "\tCurrent Loss: 0.1188\n",
      "\tCurrent Loss: 0.1466\n",
      "\tCurrent Loss: 0.1262\n",
      "\tCurrent Loss: 0.1356\n",
      "\tCurrent Loss: 0.1190\n",
      "\tCurrent Loss: 0.1220\n",
      "Train Epoch 9 Train Loss: 0.10521124429702759 Valid Acc: 82.21%\n",
      "\tCurrent Loss: 0.0761\n",
      "\tCurrent Loss: 0.0770\n",
      "\tCurrent Loss: 0.0530\n",
      "\tCurrent Loss: 0.0466\n",
      "\tCurrent Loss: 0.0651\n",
      "\tCurrent Loss: 0.0651\n",
      "\tCurrent Loss: 0.0698\n",
      "\tCurrent Loss: 0.0933\n",
      "\tCurrent Loss: 0.0653\n",
      "\tCurrent Loss: 0.0692\n",
      "\tCurrent Loss: 0.0637\n",
      "\tCurrent Loss: 0.0857\n",
      "\tCurrent Loss: 0.0830\n",
      "\tCurrent Loss: 0.0857\n",
      "\tCurrent Loss: 0.0888\n",
      "\tCurrent Loss: 0.1066\n",
      "\tCurrent Loss: 0.1128\n",
      "\tCurrent Loss: 0.1167\n",
      "\tCurrent Loss: 0.1295\n",
      "\tCurrent Loss: 0.1260\n",
      "Train Epoch 10 Train Loss: 0.08524191798567772 Valid Acc: 82.06%\n",
      "\tCurrent Loss: 0.0725\n",
      "\tCurrent Loss: 0.0665\n",
      "\tCurrent Loss: 0.0418\n",
      "\tCurrent Loss: 0.0459\n",
      "\tCurrent Loss: 0.0515\n",
      "\tCurrent Loss: 0.0517\n",
      "\tCurrent Loss: 0.0458\n",
      "\tCurrent Loss: 0.0387\n",
      "\tCurrent Loss: 0.0543\n",
      "\tCurrent Loss: 0.0782\n",
      "\tCurrent Loss: 0.0606\n",
      "\tCurrent Loss: 0.0761\n",
      "\tCurrent Loss: 0.0776\n",
      "\tCurrent Loss: 0.0665\n",
      "\tCurrent Loss: 0.0737\n",
      "\tCurrent Loss: 0.0568\n",
      "\tCurrent Loss: 0.0598\n",
      "\tCurrent Loss: 0.0990\n",
      "\tCurrent Loss: 0.0707\n",
      "\tCurrent Loss: 0.0699\n",
      "Train Epoch 11 Train Loss: 0.06405064069479705 Valid Acc: 82.48%\n",
      "\tCurrent Loss: 0.0619\n",
      "\tCurrent Loss: 0.0719\n",
      "\tCurrent Loss: 0.0618\n",
      "\tCurrent Loss: 0.0459\n",
      "\tCurrent Loss: 0.0395\n",
      "\tCurrent Loss: 0.0364\n",
      "\tCurrent Loss: 0.0451\n",
      "\tCurrent Loss: 0.0438\n",
      "\tCurrent Loss: 0.0568\n",
      "\tCurrent Loss: 0.0429\n",
      "\tCurrent Loss: 0.0450\n",
      "\tCurrent Loss: 0.0393\n",
      "\tCurrent Loss: 0.0807\n",
      "\tCurrent Loss: 0.0737\n",
      "\tCurrent Loss: 0.0689\n",
      "\tCurrent Loss: 0.0674\n",
      "\tCurrent Loss: 0.0640\n",
      "\tCurrent Loss: 0.0720\n",
      "\tCurrent Loss: 0.0794\n",
      "\tCurrent Loss: 0.0732\n",
      "Train Epoch 12 Train Loss: 0.05892199099212885 Valid Acc: 82.62%\n",
      "\tCurrent Loss: 0.0417\n",
      "\tCurrent Loss: 0.0359\n",
      "\tCurrent Loss: 0.0397\n",
      "\tCurrent Loss: 0.0497\n",
      "\tCurrent Loss: 0.0630\n",
      "\tCurrent Loss: 0.0638\n",
      "\tCurrent Loss: 0.0533\n",
      "\tCurrent Loss: 0.0606\n",
      "\tCurrent Loss: 0.0617\n",
      "\tCurrent Loss: 0.0480\n",
      "\tCurrent Loss: 0.0706\n",
      "\tCurrent Loss: 0.0620\n",
      "\tCurrent Loss: 0.0775\n",
      "\tCurrent Loss: 0.0749\n",
      "\tCurrent Loss: 0.0677\n",
      "\tCurrent Loss: 0.0693\n",
      "\tCurrent Loss: 0.0594\n",
      "\tCurrent Loss: 0.0662\n",
      "\tCurrent Loss: 0.0636\n",
      "\tCurrent Loss: 0.0782\n",
      "Train Epoch 13 Train Loss: 0.0605695975586772 Valid Acc: 81.57%\n",
      "\tCurrent Loss: 0.0511\n",
      "\tCurrent Loss: 0.0354\n",
      "\tCurrent Loss: 0.0262\n",
      "\tCurrent Loss: 0.0321\n",
      "\tCurrent Loss: 0.0348\n",
      "\tCurrent Loss: 0.0403\n",
      "\tCurrent Loss: 0.0425\n",
      "\tCurrent Loss: 0.0368\n",
      "\tCurrent Loss: 0.0282\n",
      "\tCurrent Loss: 0.0414\n",
      "\tCurrent Loss: 0.0381\n",
      "\tCurrent Loss: 0.0569\n",
      "\tCurrent Loss: 0.0670\n",
      "\tCurrent Loss: 0.0549\n",
      "\tCurrent Loss: 0.0684\n",
      "\tCurrent Loss: 0.0690\n",
      "\tCurrent Loss: 0.0697\n",
      "\tCurrent Loss: 0.0643\n",
      "\tCurrent Loss: 0.0491\n",
      "\tCurrent Loss: 0.0516\n",
      "Train Epoch 14 Train Loss: 0.04830593764185905 Valid Acc: 83.23%\n",
      "\tCurrent Loss: 0.0473\n",
      "\tCurrent Loss: 0.0359\n",
      "\tCurrent Loss: 0.0413\n",
      "\tCurrent Loss: 0.0365\n",
      "\tCurrent Loss: 0.0365\n",
      "\tCurrent Loss: 0.0437\n",
      "\tCurrent Loss: 0.0457\n",
      "\tCurrent Loss: 0.0345\n",
      "\tCurrent Loss: 0.0438\n",
      "\tCurrent Loss: 0.0366\n",
      "\tCurrent Loss: 0.0518\n",
      "\tCurrent Loss: 0.0291\n",
      "\tCurrent Loss: 0.0559\n",
      "\tCurrent Loss: 0.0611\n",
      "\tCurrent Loss: 0.0531\n",
      "\tCurrent Loss: 0.0396\n",
      "\tCurrent Loss: 0.0465\n",
      "\tCurrent Loss: 0.0513\n",
      "\tCurrent Loss: 0.0552\n",
      "\tCurrent Loss: 0.0682\n",
      "Train Epoch 15 Train Loss: 0.04569299527853728 Valid Acc: 82.01%\n",
      "\tCurrent Loss: 0.0368\n",
      "\tCurrent Loss: 0.0364\n",
      "\tCurrent Loss: 0.0328\n",
      "\tCurrent Loss: 0.0354\n",
      "\tCurrent Loss: 0.0370\n",
      "\tCurrent Loss: 0.0329\n",
      "\tCurrent Loss: 0.0372\n",
      "\tCurrent Loss: 0.0463\n",
      "\tCurrent Loss: 0.0397\n",
      "\tCurrent Loss: 0.0394\n",
      "\tCurrent Loss: 0.0406\n",
      "\tCurrent Loss: 0.0429\n",
      "\tCurrent Loss: 0.0364\n",
      "\tCurrent Loss: 0.0522\n",
      "\tCurrent Loss: 0.0415\n",
      "\tCurrent Loss: 0.0451\n",
      "\tCurrent Loss: 0.0546\n",
      "\tCurrent Loss: 0.0501\n",
      "\tCurrent Loss: 0.0334\n",
      "\tCurrent Loss: 0.0540\n",
      "Train Epoch 16 Train Loss: 0.04124857962280512 Valid Acc: 82.54%\n",
      "\tCurrent Loss: 0.0332\n",
      "\tCurrent Loss: 0.0199\n",
      "\tCurrent Loss: 0.0211\n",
      "\tCurrent Loss: 0.0290\n",
      "\tCurrent Loss: 0.0350\n",
      "\tCurrent Loss: 0.0348\n",
      "\tCurrent Loss: 0.0334\n",
      "\tCurrent Loss: 0.0436\n",
      "\tCurrent Loss: 0.0295\n",
      "\tCurrent Loss: 0.0371\n",
      "\tCurrent Loss: 0.0584\n",
      "\tCurrent Loss: 0.0413\n",
      "\tCurrent Loss: 0.0515\n",
      "\tCurrent Loss: 0.0563\n",
      "\tCurrent Loss: 0.0553\n",
      "\tCurrent Loss: 0.0349\n",
      "\tCurrent Loss: 0.0305\n",
      "\tCurrent Loss: 0.0348\n",
      "\tCurrent Loss: 0.0385\n",
      "\tCurrent Loss: 0.0441\n",
      "Train Epoch 17 Train Loss: 0.038311353020370004 Valid Acc: 83.34%\n",
      "\tCurrent Loss: 0.0309\n",
      "\tCurrent Loss: 0.0358\n",
      "\tCurrent Loss: 0.0437\n",
      "\tCurrent Loss: 0.0270\n",
      "\tCurrent Loss: 0.0359\n",
      "\tCurrent Loss: 0.0381\n",
      "\tCurrent Loss: 0.0373\n",
      "\tCurrent Loss: 0.0372\n",
      "\tCurrent Loss: 0.0536\n",
      "\tCurrent Loss: 0.0639\n",
      "\tCurrent Loss: 0.0465\n",
      "\tCurrent Loss: 0.0382\n",
      "\tCurrent Loss: 0.0426\n",
      "\tCurrent Loss: 0.0587\n",
      "\tCurrent Loss: 0.0691\n",
      "\tCurrent Loss: 0.0714\n",
      "\tCurrent Loss: 0.0575\n",
      "\tCurrent Loss: 0.0545\n",
      "\tCurrent Loss: 0.0570\n",
      "\tCurrent Loss: 0.0362\n",
      "Train Epoch 18 Train Loss: 0.04638082741796971 Valid Acc: 83.06%\n",
      "\tCurrent Loss: 0.0237\n",
      "\tCurrent Loss: 0.0190\n",
      "\tCurrent Loss: 0.0331\n",
      "\tCurrent Loss: 0.0206\n",
      "\tCurrent Loss: 0.0229\n",
      "\tCurrent Loss: 0.0239\n",
      "\tCurrent Loss: 0.0213\n",
      "\tCurrent Loss: 0.0238\n",
      "\tCurrent Loss: 0.0291\n",
      "\tCurrent Loss: 0.0452\n",
      "\tCurrent Loss: 0.0480\n",
      "\tCurrent Loss: 0.0543\n",
      "\tCurrent Loss: 0.0419\n",
      "\tCurrent Loss: 0.0423\n",
      "\tCurrent Loss: 0.0542\n",
      "\tCurrent Loss: 0.0415\n",
      "\tCurrent Loss: 0.0404\n",
      "\tCurrent Loss: 0.0476\n",
      "\tCurrent Loss: 0.0429\n",
      "\tCurrent Loss: 0.0298\n",
      "Train Epoch 19 Train Loss: 0.03553988308310509 Valid Acc: 83.82%\n"
     ]
    }
   ],
   "source": [
    "best_valid_acc = -1\n",
    "optimizer = torch.optim.Adam(official_resnet18.parameters())\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "   \n",
    "    train_loss = train(official_resnet18, train_loader, optimizer, criterion)\n",
    "    valid_acc = evaluate(official_resnet18, valid_loader)\n",
    "    \n",
    "    print('Train Epoch {} Train Loss: {} Valid Acc: {}%'.format(epoch, train_loss, valid_acc))\n",
    "    \n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        torch.save(official_resnet18.state_dict(), 'official_resnet18.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "DcWvCp8JBp1B"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of total 10000 images: 83.77%\n"
     ]
    }
   ],
   "source": [
    "official_resnet18.load_state_dict(torch.load('official_resnet18.pth'))\n",
    "test(official_resnet18, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "official_resnet50.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "7nzQLwF3Bp1F"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tCurrent Loss: 2.4300\n",
      "\tCurrent Loss: 1.9947\n",
      "\tCurrent Loss: 1.9188\n",
      "\tCurrent Loss: 1.8566\n",
      "\tCurrent Loss: 1.8193\n",
      "\tCurrent Loss: 1.7579\n",
      "\tCurrent Loss: 1.7678\n",
      "\tCurrent Loss: 1.6738\n",
      "\tCurrent Loss: 1.6739\n",
      "\tCurrent Loss: 1.6455\n",
      "\tCurrent Loss: 1.5805\n",
      "\tCurrent Loss: 1.5963\n",
      "\tCurrent Loss: 1.4607\n",
      "\tCurrent Loss: 1.4807\n",
      "\tCurrent Loss: 1.4927\n",
      "\tCurrent Loss: 1.4394\n",
      "\tCurrent Loss: 1.3930\n",
      "\tCurrent Loss: 1.4134\n",
      "\tCurrent Loss: 1.2925\n",
      "\tCurrent Loss: 1.3226\n",
      "Train Epoch 0 Train Loss: 1.637575026321411 Valid Acc: 49.0%\n",
      "\tCurrent Loss: 1.2596\n",
      "\tCurrent Loss: 1.3304\n",
      "\tCurrent Loss: 1.2356\n",
      "\tCurrent Loss: 1.2171\n",
      "\tCurrent Loss: 1.1899\n",
      "\tCurrent Loss: 1.2334\n",
      "\tCurrent Loss: 1.1533\n",
      "\tCurrent Loss: 1.1363\n",
      "\tCurrent Loss: 1.1035\n",
      "\tCurrent Loss: 1.1550\n",
      "\tCurrent Loss: 1.1218\n",
      "\tCurrent Loss: 1.1235\n",
      "\tCurrent Loss: 1.0699\n",
      "\tCurrent Loss: 1.0150\n",
      "\tCurrent Loss: 1.0865\n",
      "\tCurrent Loss: 1.0783\n",
      "\tCurrent Loss: 0.9764\n",
      "\tCurrent Loss: 1.0424\n",
      "\tCurrent Loss: 1.0007\n",
      "\tCurrent Loss: 0.9792\n",
      "Train Epoch 1 Train Loss: 1.122745821762085 Valid Acc: 61.55%\n",
      "\tCurrent Loss: 0.8993\n",
      "\tCurrent Loss: 0.9476\n",
      "\tCurrent Loss: 0.9343\n",
      "\tCurrent Loss: 0.8939\n",
      "\tCurrent Loss: 0.8967\n",
      "\tCurrent Loss: 0.8795\n",
      "\tCurrent Loss: 0.8687\n",
      "\tCurrent Loss: 0.8656\n",
      "\tCurrent Loss: 0.8881\n",
      "\tCurrent Loss: 0.9289\n",
      "\tCurrent Loss: 0.9171\n",
      "\tCurrent Loss: 0.8986\n",
      "\tCurrent Loss: 0.9425\n",
      "\tCurrent Loss: 0.8820\n",
      "\tCurrent Loss: 0.8573\n",
      "\tCurrent Loss: 0.8636\n",
      "\tCurrent Loss: 0.8421\n",
      "\tCurrent Loss: 0.8796\n",
      "\tCurrent Loss: 0.8408\n",
      "\tCurrent Loss: 0.8419\n",
      "Train Epoch 2 Train Loss: 0.8834044922828674 Valid Acc: 67.25%\n",
      "\tCurrent Loss: 0.7785\n",
      "\tCurrent Loss: 0.7583\n",
      "\tCurrent Loss: 0.7825\n",
      "\tCurrent Loss: 0.7897\n",
      "\tCurrent Loss: 0.7213\n",
      "\tCurrent Loss: 0.7672\n",
      "\tCurrent Loss: 0.7657\n",
      "\tCurrent Loss: 0.7445\n",
      "\tCurrent Loss: 0.7306\n",
      "\tCurrent Loss: 0.7493\n",
      "\tCurrent Loss: 0.7082\n",
      "\tCurrent Loss: 0.7474\n",
      "\tCurrent Loss: 0.7397\n",
      "\tCurrent Loss: 0.7087\n",
      "\tCurrent Loss: 0.7250\n",
      "\tCurrent Loss: 0.6910\n",
      "\tCurrent Loss: 0.7255\n",
      "\tCurrent Loss: 0.6868\n",
      "\tCurrent Loss: 0.7165\n",
      "\tCurrent Loss: 0.6814\n",
      "Train Epoch 3 Train Loss: 0.734973619222641 Valid Acc: 71.53%\n",
      "\tCurrent Loss: 0.5861\n",
      "\tCurrent Loss: 0.6603\n",
      "\tCurrent Loss: 0.6731\n",
      "\tCurrent Loss: 0.6193\n",
      "\tCurrent Loss: 0.6248\n",
      "\tCurrent Loss: 0.6022\n",
      "\tCurrent Loss: 0.6032\n",
      "\tCurrent Loss: 0.6389\n",
      "\tCurrent Loss: 0.6191\n",
      "\tCurrent Loss: 0.6010\n",
      "\tCurrent Loss: 0.6320\n",
      "\tCurrent Loss: 0.6093\n",
      "\tCurrent Loss: 0.5987\n",
      "\tCurrent Loss: 0.6120\n",
      "\tCurrent Loss: 0.5708\n",
      "\tCurrent Loss: 0.5830\n",
      "\tCurrent Loss: 0.6052\n",
      "\tCurrent Loss: 0.6275\n",
      "\tCurrent Loss: 0.5633\n",
      "\tCurrent Loss: 0.6033\n",
      "Train Epoch 4 Train Loss: 0.610735232591629 Valid Acc: 71.43%\n",
      "\tCurrent Loss: 0.4973\n",
      "\tCurrent Loss: 0.5025\n",
      "\tCurrent Loss: 0.4989\n",
      "\tCurrent Loss: 0.5078\n",
      "\tCurrent Loss: 0.5450\n",
      "\tCurrent Loss: 0.5104\n",
      "\tCurrent Loss: 0.5314\n",
      "\tCurrent Loss: 0.5785\n",
      "\tCurrent Loss: 0.5384\n",
      "\tCurrent Loss: 0.5291\n",
      "\tCurrent Loss: 0.4845\n",
      "\tCurrent Loss: 0.4910\n",
      "\tCurrent Loss: 0.5586\n",
      "\tCurrent Loss: 0.5612\n",
      "\tCurrent Loss: 0.4749\n",
      "\tCurrent Loss: 0.5088\n",
      "\tCurrent Loss: 0.5502\n",
      "\tCurrent Loss: 0.5183\n",
      "\tCurrent Loss: 0.4853\n",
      "\tCurrent Loss: 0.5160\n",
      "Train Epoch 5 Train Loss: 0.5205626974105835 Valid Acc: 74.57%\n",
      "\tCurrent Loss: 0.3916\n",
      "\tCurrent Loss: 0.4337\n",
      "\tCurrent Loss: 0.4281\n",
      "\tCurrent Loss: 0.4638\n",
      "\tCurrent Loss: 0.4083\n",
      "\tCurrent Loss: 0.4416\n",
      "\tCurrent Loss: 0.4746\n",
      "\tCurrent Loss: 0.4223\n",
      "\tCurrent Loss: 0.4475\n",
      "\tCurrent Loss: 0.4316\n",
      "\tCurrent Loss: 0.4322\n",
      "\tCurrent Loss: 0.4341\n",
      "\tCurrent Loss: 0.4714\n",
      "\tCurrent Loss: 0.4352\n",
      "\tCurrent Loss: 0.4669\n",
      "\tCurrent Loss: 0.4335\n",
      "\tCurrent Loss: 0.4431\n",
      "\tCurrent Loss: 0.5051\n",
      "\tCurrent Loss: 0.4133\n",
      "\tCurrent Loss: 0.4402\n",
      "Train Epoch 6 Train Loss: 0.4404481219291687 Valid Acc: 80.77%\n",
      "\tCurrent Loss: 0.3486\n",
      "\tCurrent Loss: 0.3498\n",
      "\tCurrent Loss: 0.3676\n",
      "\tCurrent Loss: 0.3716\n",
      "\tCurrent Loss: 0.3434\n",
      "\tCurrent Loss: 0.3393\n",
      "\tCurrent Loss: 0.3404\n",
      "\tCurrent Loss: 0.3562\n",
      "\tCurrent Loss: 0.3533\n",
      "\tCurrent Loss: 0.4001\n",
      "\tCurrent Loss: 0.3408\n",
      "\tCurrent Loss: 0.3645\n",
      "\tCurrent Loss: 0.3592\n",
      "\tCurrent Loss: 0.3810\n",
      "\tCurrent Loss: 0.3745\n",
      "\tCurrent Loss: 0.3843\n",
      "\tCurrent Loss: 0.4016\n",
      "\tCurrent Loss: 0.3409\n",
      "\tCurrent Loss: 0.3717\n",
      "\tCurrent Loss: 0.3902\n",
      "Train Epoch 7 Train Loss: 0.3653196821928024 Valid Acc: 78.19%\n",
      "\tCurrent Loss: 0.2494\n",
      "\tCurrent Loss: 0.2494\n",
      "\tCurrent Loss: 0.2913\n",
      "\tCurrent Loss: 0.2905\n",
      "\tCurrent Loss: 0.2781\n",
      "\tCurrent Loss: 0.3680\n",
      "\tCurrent Loss: 0.3603\n",
      "\tCurrent Loss: 0.3181\n",
      "\tCurrent Loss: 0.2881\n",
      "\tCurrent Loss: 0.3150\n",
      "\tCurrent Loss: 0.3070\n",
      "\tCurrent Loss: 0.3228\n",
      "\tCurrent Loss: 0.3265\n",
      "\tCurrent Loss: 0.3587\n",
      "\tCurrent Loss: 0.3502\n",
      "\tCurrent Loss: 0.3158\n",
      "\tCurrent Loss: 0.2823\n",
      "\tCurrent Loss: 0.2994\n",
      "\tCurrent Loss: 0.3034\n",
      "\tCurrent Loss: 0.3018\n",
      "Train Epoch 8 Train Loss: 0.30847959452867507 Valid Acc: 74.55%\n",
      "\tCurrent Loss: 0.2349\n",
      "\tCurrent Loss: 0.2219\n",
      "\tCurrent Loss: 0.2221\n",
      "\tCurrent Loss: 0.2448\n",
      "\tCurrent Loss: 0.2557\n",
      "\tCurrent Loss: 0.2803\n",
      "\tCurrent Loss: 0.2378\n",
      "\tCurrent Loss: 0.2605\n",
      "\tCurrent Loss: 0.2444\n",
      "\tCurrent Loss: 0.2513\n",
      "\tCurrent Loss: 0.2750\n",
      "\tCurrent Loss: 0.2438\n",
      "\tCurrent Loss: 0.2381\n",
      "\tCurrent Loss: 0.2735\n",
      "\tCurrent Loss: 0.2603\n",
      "\tCurrent Loss: 0.2832\n",
      "\tCurrent Loss: 0.2647\n",
      "\tCurrent Loss: 0.2598\n",
      "\tCurrent Loss: 0.2636\n",
      "\tCurrent Loss: 0.2951\n",
      "Train Epoch 9 Train Loss: 0.2561462514638901 Valid Acc: 79.67%\n",
      "\tCurrent Loss: 0.1520\n",
      "\tCurrent Loss: 0.1328\n",
      "\tCurrent Loss: 0.1833\n",
      "\tCurrent Loss: 0.1597\n",
      "\tCurrent Loss: 0.1647\n",
      "\tCurrent Loss: 0.2097\n",
      "\tCurrent Loss: 0.2268\n",
      "\tCurrent Loss: 0.1782\n",
      "\tCurrent Loss: 0.1916\n",
      "\tCurrent Loss: 0.2063\n",
      "\tCurrent Loss: 0.2125\n",
      "\tCurrent Loss: 0.2104\n",
      "\tCurrent Loss: 0.1977\n",
      "\tCurrent Loss: 0.1926\n",
      "\tCurrent Loss: 0.2080\n",
      "\tCurrent Loss: 0.2485\n",
      "\tCurrent Loss: 0.2081\n",
      "\tCurrent Loss: 0.1981\n",
      "\tCurrent Loss: 0.2035\n",
      "\tCurrent Loss: 0.2049\n",
      "Train Epoch 10 Train Loss: 0.1961719648718834 Valid Acc: 78.98%\n",
      "\tCurrent Loss: 0.1673\n",
      "\tCurrent Loss: 0.1327\n",
      "\tCurrent Loss: 0.1501\n",
      "\tCurrent Loss: 0.1555\n",
      "\tCurrent Loss: 0.1658\n",
      "\tCurrent Loss: 0.1892\n",
      "\tCurrent Loss: 0.1705\n",
      "\tCurrent Loss: 0.1637\n",
      "\tCurrent Loss: 0.1850\n",
      "\tCurrent Loss: 0.1731\n",
      "\tCurrent Loss: 0.1513\n",
      "\tCurrent Loss: 0.1677\n",
      "\tCurrent Loss: 0.1515\n",
      "\tCurrent Loss: 0.1768\n",
      "\tCurrent Loss: 0.1638\n",
      "\tCurrent Loss: 0.1669\n",
      "\tCurrent Loss: 0.1727\n",
      "\tCurrent Loss: 0.1924\n",
      "\tCurrent Loss: 0.2026\n",
      "\tCurrent Loss: 0.1689\n",
      "Train Epoch 11 Train Loss: 0.16951150147318841 Valid Acc: 80.66%\n",
      "\tCurrent Loss: 0.1352\n",
      "\tCurrent Loss: 0.0917\n",
      "\tCurrent Loss: 0.1070\n",
      "\tCurrent Loss: 0.1253\n",
      "\tCurrent Loss: 0.1268\n",
      "\tCurrent Loss: 0.1284\n",
      "\tCurrent Loss: 0.1041\n",
      "\tCurrent Loss: 0.0994\n",
      "\tCurrent Loss: 0.1287\n",
      "\tCurrent Loss: 0.1362\n",
      "\tCurrent Loss: 0.1329\n",
      "\tCurrent Loss: 0.1374\n",
      "\tCurrent Loss: 0.1401\n",
      "\tCurrent Loss: 0.1366\n",
      "\tCurrent Loss: 0.1346\n",
      "\tCurrent Loss: 0.1261\n",
      "\tCurrent Loss: 0.1434\n",
      "\tCurrent Loss: 0.1472\n",
      "\tCurrent Loss: 0.1233\n",
      "\tCurrent Loss: 0.1675\n",
      "Train Epoch 12 Train Loss: 0.12848276834487915 Valid Acc: 82.37%\n",
      "\tCurrent Loss: 0.0813\n",
      "\tCurrent Loss: 0.0958\n",
      "\tCurrent Loss: 0.0858\n",
      "\tCurrent Loss: 0.0780\n",
      "\tCurrent Loss: 0.0858\n",
      "\tCurrent Loss: 0.1117\n",
      "\tCurrent Loss: 0.0917\n",
      "\tCurrent Loss: 0.1081\n",
      "\tCurrent Loss: 0.1227\n",
      "\tCurrent Loss: 0.1252\n",
      "\tCurrent Loss: 0.1161\n",
      "\tCurrent Loss: 0.1085\n",
      "\tCurrent Loss: 0.1352\n",
      "\tCurrent Loss: 0.1185\n",
      "\tCurrent Loss: 0.0884\n",
      "\tCurrent Loss: 0.1092\n",
      "\tCurrent Loss: 0.1215\n",
      "\tCurrent Loss: 0.1427\n",
      "\tCurrent Loss: 0.1218\n",
      "\tCurrent Loss: 0.1451\n",
      "Train Epoch 13 Train Loss: 0.11015015350282192 Valid Acc: 82.33%\n",
      "\tCurrent Loss: 0.0833\n",
      "\tCurrent Loss: 0.0818\n",
      "\tCurrent Loss: 0.0695\n",
      "\tCurrent Loss: 0.0913\n",
      "\tCurrent Loss: 0.0794\n",
      "\tCurrent Loss: 0.0723\n",
      "\tCurrent Loss: 0.1157\n",
      "\tCurrent Loss: 0.1183\n",
      "\tCurrent Loss: 0.1028\n",
      "\tCurrent Loss: 0.0936\n",
      "\tCurrent Loss: 0.1006\n",
      "\tCurrent Loss: 0.0960\n",
      "\tCurrent Loss: 0.0990\n",
      "\tCurrent Loss: 0.1334\n",
      "\tCurrent Loss: 0.1590\n",
      "\tCurrent Loss: 0.1207\n",
      "\tCurrent Loss: 0.1219\n",
      "\tCurrent Loss: 0.1419\n",
      "\tCurrent Loss: 0.1503\n",
      "\tCurrent Loss: 0.1056\n",
      "Train Epoch 14 Train Loss: 0.10687150744646788 Valid Acc: 79.5%\n",
      "\tCurrent Loss: 0.0521\n",
      "\tCurrent Loss: 0.0708\n",
      "\tCurrent Loss: 0.0597\n",
      "\tCurrent Loss: 0.0538\n",
      "\tCurrent Loss: 0.0757\n",
      "\tCurrent Loss: 0.0719\n",
      "\tCurrent Loss: 0.0579\n",
      "\tCurrent Loss: 0.0533\n",
      "\tCurrent Loss: 0.0820\n",
      "\tCurrent Loss: 0.0767\n",
      "\tCurrent Loss: 0.1150\n",
      "\tCurrent Loss: 0.0927\n",
      "\tCurrent Loss: 0.0885\n",
      "\tCurrent Loss: 0.0972\n",
      "\tCurrent Loss: 0.1033\n",
      "\tCurrent Loss: 0.1099\n",
      "\tCurrent Loss: 0.0892\n",
      "\tCurrent Loss: 0.1001\n",
      "\tCurrent Loss: 0.0921\n",
      "\tCurrent Loss: 0.0773\n",
      "Train Epoch 15 Train Loss: 0.08236085549294948 Valid Acc: 79.56%\n",
      "\tCurrent Loss: 0.0743\n",
      "\tCurrent Loss: 0.0560\n",
      "\tCurrent Loss: 0.0655\n",
      "\tCurrent Loss: 0.0901\n",
      "\tCurrent Loss: 0.0615\n",
      "\tCurrent Loss: 0.0552\n",
      "\tCurrent Loss: 0.0530\n",
      "\tCurrent Loss: 0.0380\n",
      "\tCurrent Loss: 0.0567\n",
      "\tCurrent Loss: 0.0494\n",
      "\tCurrent Loss: 0.0571\n",
      "\tCurrent Loss: 0.0540\n",
      "\tCurrent Loss: 0.0676\n",
      "\tCurrent Loss: 0.1147\n",
      "\tCurrent Loss: 0.0805\n",
      "\tCurrent Loss: 0.1047\n",
      "\tCurrent Loss: 0.0879\n",
      "\tCurrent Loss: 0.1168\n",
      "\tCurrent Loss: 0.1042\n",
      "\tCurrent Loss: 0.1050\n",
      "Train Epoch 16 Train Loss: 0.07501724993661046 Valid Acc: 81.78%\n",
      "\tCurrent Loss: 0.0516\n",
      "\tCurrent Loss: 0.0536\n",
      "\tCurrent Loss: 0.0458\n",
      "\tCurrent Loss: 0.0759\n",
      "\tCurrent Loss: 0.0683\n",
      "\tCurrent Loss: 0.0607\n",
      "\tCurrent Loss: 0.0764\n",
      "\tCurrent Loss: 0.0585\n",
      "\tCurrent Loss: 0.0773\n",
      "\tCurrent Loss: 0.0763\n",
      "\tCurrent Loss: 0.1119\n",
      "\tCurrent Loss: 0.0630\n",
      "\tCurrent Loss: 0.0848\n",
      "\tCurrent Loss: 0.0849\n",
      "\tCurrent Loss: 0.0771\n",
      "\tCurrent Loss: 0.1117\n",
      "\tCurrent Loss: 0.1091\n",
      "\tCurrent Loss: 0.1138\n",
      "\tCurrent Loss: 0.0901\n",
      "\tCurrent Loss: 0.0736\n",
      "Train Epoch 17 Train Loss: 0.07789306939393283 Valid Acc: 82.34%\n",
      "\tCurrent Loss: 0.0424\n",
      "\tCurrent Loss: 0.0600\n",
      "\tCurrent Loss: 0.0729\n",
      "\tCurrent Loss: 0.0516\n",
      "\tCurrent Loss: 0.0439\n",
      "\tCurrent Loss: 0.0494\n",
      "\tCurrent Loss: 0.0553\n",
      "\tCurrent Loss: 0.0514\n",
      "\tCurrent Loss: 0.0575\n",
      "\tCurrent Loss: 0.0741\n",
      "\tCurrent Loss: 0.0631\n",
      "\tCurrent Loss: 0.0773\n",
      "\tCurrent Loss: 0.0651\n",
      "\tCurrent Loss: 0.0711\n",
      "\tCurrent Loss: 0.0781\n",
      "\tCurrent Loss: 0.0823\n",
      "\tCurrent Loss: 0.0797\n",
      "\tCurrent Loss: 0.0795\n",
      "\tCurrent Loss: 0.0997\n",
      "\tCurrent Loss: 0.0946\n",
      "Train Epoch 18 Train Loss: 0.06779712756499648 Valid Acc: 81.31%\n",
      "\tCurrent Loss: 0.0678\n",
      "\tCurrent Loss: 0.0388\n",
      "\tCurrent Loss: 0.0352\n",
      "\tCurrent Loss: 0.0495\n",
      "\tCurrent Loss: 0.0454\n",
      "\tCurrent Loss: 0.0773\n",
      "\tCurrent Loss: 0.0831\n",
      "\tCurrent Loss: 0.0740\n",
      "\tCurrent Loss: 0.0599\n",
      "\tCurrent Loss: 0.0583\n",
      "\tCurrent Loss: 0.0917\n",
      "\tCurrent Loss: 0.0714\n",
      "\tCurrent Loss: 0.0587\n",
      "\tCurrent Loss: 0.0613\n",
      "\tCurrent Loss: 0.0667\n",
      "\tCurrent Loss: 0.0423\n",
      "\tCurrent Loss: 0.0823\n",
      "\tCurrent Loss: 0.0656\n",
      "\tCurrent Loss: 0.0871\n",
      "\tCurrent Loss: 0.0659\n",
      "Train Epoch 19 Train Loss: 0.06356028753817082 Valid Acc: 82.66%\n"
     ]
    }
   ],
   "source": [
    "best_valid_acc = -1\n",
    "optimizer = torch.optim.Adam(official_resnet50.parameters())\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "   \n",
    "    train_loss = train(official_resnet50, train_loader, optimizer, criterion)\n",
    "    valid_acc = evaluate(official_resnet50, valid_loader)\n",
    "    \n",
    "    print('Train Epoch {} Train Loss: {} Valid Acc: {}%'.format(epoch, train_loss, valid_acc))\n",
    "    \n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        torch.save(official_resnet50.state_dict(), 'official_resnet50.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "rYd62uSRBp1I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of total 10000 images: 83.42%\n"
     ]
    }
   ],
   "source": [
    "official_resnet50.load_state_dict(torch.load('official_resnet50.pth'))\n",
    "test(official_resnet50, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet50(\n",
       "  (conv): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "  (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (maxpool2d): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layers): Sequential(\n",
       "    (0): BottleNeck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BottleNeck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): BottleNeck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (3): BottleNeck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (4): BottleNeck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (5): BottleNeck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (6): BottleNeck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (7): BottleNeck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (8): BottleNeck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (9): BottleNeck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (10): BottleNeck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (11): BottleNeck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (12): BottleNeck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (13): BottleNeck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (14): BottleNeck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (15): BottleNeck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (avgpool2d): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
       "  (fc): Linear(in_features=2048, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "custom_resnet50.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "TQdhgghgBp1L",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tCurrent Loss: 2.4353\n",
      "\tCurrent Loss: 2.1022\n",
      "\tCurrent Loss: 1.9107\n",
      "\tCurrent Loss: 1.8788\n",
      "\tCurrent Loss: 1.8638\n",
      "\tCurrent Loss: 1.8533\n",
      "\tCurrent Loss: 1.7530\n",
      "\tCurrent Loss: 1.7616\n",
      "\tCurrent Loss: 1.7367\n",
      "\tCurrent Loss: 1.6974\n",
      "\tCurrent Loss: 1.6478\n",
      "\tCurrent Loss: 1.6545\n",
      "\tCurrent Loss: 1.6548\n",
      "\tCurrent Loss: 1.6525\n",
      "\tCurrent Loss: 1.6008\n",
      "\tCurrent Loss: 1.5538\n",
      "\tCurrent Loss: 1.4828\n",
      "\tCurrent Loss: 1.4587\n",
      "\tCurrent Loss: 1.5315\n",
      "\tCurrent Loss: 1.4594\n",
      "Train Epoch 0 Train Loss: 1.7207319160461425 Valid Acc: 47.75%\n",
      "\tCurrent Loss: 1.4077\n",
      "\tCurrent Loss: 1.3250\n",
      "\tCurrent Loss: 1.3381\n",
      "\tCurrent Loss: 1.3222\n",
      "\tCurrent Loss: 1.2501\n",
      "\tCurrent Loss: 1.2958\n",
      "\tCurrent Loss: 1.2306\n",
      "\tCurrent Loss: 1.2643\n",
      "\tCurrent Loss: 1.2280\n",
      "\tCurrent Loss: 1.2686\n",
      "\tCurrent Loss: 1.2641\n",
      "\tCurrent Loss: 1.2009\n",
      "\tCurrent Loss: 1.1924\n",
      "\tCurrent Loss: 1.1977\n",
      "\tCurrent Loss: 1.2016\n",
      "\tCurrent Loss: 1.1219\n",
      "\tCurrent Loss: 1.1293\n",
      "\tCurrent Loss: 1.1038\n",
      "\tCurrent Loss: 1.1449\n",
      "\tCurrent Loss: 1.1084\n",
      "Train Epoch 1 Train Loss: 1.2245042080879212 Valid Acc: 58.21%\n",
      "\tCurrent Loss: 1.0722\n",
      "\tCurrent Loss: 1.0415\n",
      "\tCurrent Loss: 1.0383\n",
      "\tCurrent Loss: 1.0937\n",
      "\tCurrent Loss: 1.0625\n",
      "\tCurrent Loss: 1.0586\n",
      "\tCurrent Loss: 1.0374\n",
      "\tCurrent Loss: 0.9959\n",
      "\tCurrent Loss: 0.9781\n",
      "\tCurrent Loss: 1.0511\n",
      "\tCurrent Loss: 1.0226\n",
      "\tCurrent Loss: 0.9835\n",
      "\tCurrent Loss: 1.0143\n",
      "\tCurrent Loss: 0.9615\n",
      "\tCurrent Loss: 1.0585\n",
      "\tCurrent Loss: 0.9885\n",
      "\tCurrent Loss: 0.9534\n",
      "\tCurrent Loss: 0.9445\n",
      "\tCurrent Loss: 0.9428\n",
      "\tCurrent Loss: 0.9089\n",
      "Train Epoch 2 Train Loss: 1.0083795766830443 Valid Acc: 62.98%\n",
      "\tCurrent Loss: 0.9792\n",
      "\tCurrent Loss: 0.9104\n",
      "\tCurrent Loss: 0.8995\n",
      "\tCurrent Loss: 0.9759\n",
      "\tCurrent Loss: 0.9344\n",
      "\tCurrent Loss: 0.8680\n",
      "\tCurrent Loss: 0.8925\n",
      "\tCurrent Loss: 0.8471\n",
      "\tCurrent Loss: 0.8661\n",
      "\tCurrent Loss: 0.9070\n",
      "\tCurrent Loss: 0.8817\n",
      "\tCurrent Loss: 0.8872\n",
      "\tCurrent Loss: 0.8317\n",
      "\tCurrent Loss: 0.9092\n",
      "\tCurrent Loss: 0.8352\n",
      "\tCurrent Loss: 0.8494\n",
      "\tCurrent Loss: 0.8328\n",
      "\tCurrent Loss: 0.8568\n",
      "\tCurrent Loss: 0.8342\n",
      "\tCurrent Loss: 0.8257\n",
      "Train Epoch 3 Train Loss: 0.8810260786056519 Valid Acc: 64.62%\n",
      "\tCurrent Loss: 0.7743\n",
      "\tCurrent Loss: 0.8220\n",
      "\tCurrent Loss: 0.7941\n",
      "\tCurrent Loss: 0.8528\n",
      "\tCurrent Loss: 0.7700\n",
      "\tCurrent Loss: 0.7397\n",
      "\tCurrent Loss: 0.7873\n",
      "\tCurrent Loss: 0.7541\n",
      "\tCurrent Loss: 0.8053\n",
      "\tCurrent Loss: 0.7453\n",
      "\tCurrent Loss: 0.7597\n",
      "\tCurrent Loss: 0.7861\n",
      "\tCurrent Loss: 0.7499\n",
      "\tCurrent Loss: 0.7517\n",
      "\tCurrent Loss: 0.7350\n",
      "\tCurrent Loss: 0.7806\n",
      "\tCurrent Loss: 0.7134\n",
      "\tCurrent Loss: 0.7551\n",
      "\tCurrent Loss: 0.7943\n",
      "\tCurrent Loss: 0.6941\n",
      "Train Epoch 4 Train Loss: 0.7668853857040405 Valid Acc: 74.03%\n",
      "\tCurrent Loss: 0.6907\n",
      "\tCurrent Loss: 0.6746\n",
      "\tCurrent Loss: 0.6715\n",
      "\tCurrent Loss: 0.6864\n",
      "\tCurrent Loss: 0.6580\n",
      "\tCurrent Loss: 0.6811\n",
      "\tCurrent Loss: 0.6731\n",
      "\tCurrent Loss: 0.6877\n",
      "\tCurrent Loss: 0.6439\n",
      "\tCurrent Loss: 0.6766\n",
      "\tCurrent Loss: 0.7160\n",
      "\tCurrent Loss: 0.6797\n",
      "\tCurrent Loss: 0.6834\n",
      "\tCurrent Loss: 0.7184\n",
      "\tCurrent Loss: 0.7070\n",
      "\tCurrent Loss: 0.7040\n",
      "\tCurrent Loss: 0.6874\n",
      "\tCurrent Loss: 0.6685\n",
      "\tCurrent Loss: 0.6602\n",
      "\tCurrent Loss: 0.6576\n",
      "Train Epoch 5 Train Loss: 0.6819612333774566 Valid Acc: 72.87%\n",
      "\tCurrent Loss: 0.6429\n",
      "\tCurrent Loss: 0.6399\n",
      "\tCurrent Loss: 0.6221\n",
      "\tCurrent Loss: 0.6351\n",
      "\tCurrent Loss: 0.5939\n",
      "\tCurrent Loss: 0.5847\n",
      "\tCurrent Loss: 0.5827\n",
      "\tCurrent Loss: 0.5908\n",
      "\tCurrent Loss: 0.6073\n",
      "\tCurrent Loss: 0.5752\n",
      "\tCurrent Loss: 0.5926\n",
      "\tCurrent Loss: 0.5876\n",
      "\tCurrent Loss: 0.6327\n",
      "\tCurrent Loss: 0.6300\n",
      "\tCurrent Loss: 0.6239\n",
      "\tCurrent Loss: 0.5589\n",
      "\tCurrent Loss: 0.5932\n",
      "\tCurrent Loss: 0.5880\n",
      "\tCurrent Loss: 0.5476\n",
      "\tCurrent Loss: 0.5649\n",
      "Train Epoch 6 Train Loss: 0.6008922117710114 Valid Acc: 71.5%\n",
      "\tCurrent Loss: 0.5560\n",
      "\tCurrent Loss: 0.5640\n",
      "\tCurrent Loss: 0.5192\n",
      "\tCurrent Loss: 0.5117\n",
      "\tCurrent Loss: 0.5811\n",
      "\tCurrent Loss: 0.5370\n",
      "\tCurrent Loss: 0.5177\n",
      "\tCurrent Loss: 0.5687\n",
      "\tCurrent Loss: 0.5405\n",
      "\tCurrent Loss: 0.5195\n",
      "\tCurrent Loss: 0.5405\n",
      "\tCurrent Loss: 0.5114\n",
      "\tCurrent Loss: 0.5506\n",
      "\tCurrent Loss: 0.5211\n",
      "\tCurrent Loss: 0.5109\n",
      "\tCurrent Loss: 0.5400\n",
      "\tCurrent Loss: 0.5409\n",
      "\tCurrent Loss: 0.5575\n",
      "\tCurrent Loss: 0.5329\n",
      "\tCurrent Loss: 0.5207\n",
      "Train Epoch 7 Train Loss: 0.5368939897060394 Valid Acc: 76.28%\n",
      "\tCurrent Loss: 0.4840\n",
      "\tCurrent Loss: 0.4461\n",
      "\tCurrent Loss: 0.4427\n",
      "\tCurrent Loss: 0.4261\n",
      "\tCurrent Loss: 0.4725\n",
      "\tCurrent Loss: 0.4549\n",
      "\tCurrent Loss: 0.4744\n",
      "\tCurrent Loss: 0.4650\n",
      "\tCurrent Loss: 0.5265\n",
      "\tCurrent Loss: 0.4940\n",
      "\tCurrent Loss: 0.4517\n",
      "\tCurrent Loss: 0.4502\n",
      "\tCurrent Loss: 0.4548\n",
      "\tCurrent Loss: 0.4901\n",
      "\tCurrent Loss: 0.4530\n",
      "\tCurrent Loss: 0.5022\n",
      "\tCurrent Loss: 0.4687\n",
      "\tCurrent Loss: 0.4886\n",
      "\tCurrent Loss: 0.5076\n",
      "\tCurrent Loss: 0.4782\n",
      "Train Epoch 8 Train Loss: 0.47172356431484225 Valid Acc: 78.56%\n",
      "\tCurrent Loss: 0.3723\n",
      "\tCurrent Loss: 0.4167\n",
      "\tCurrent Loss: 0.3868\n",
      "\tCurrent Loss: 0.4010\n",
      "\tCurrent Loss: 0.4087\n",
      "\tCurrent Loss: 0.4150\n",
      "\tCurrent Loss: 0.4048\n",
      "\tCurrent Loss: 0.3985\n",
      "\tCurrent Loss: 0.4380\n",
      "\tCurrent Loss: 0.4065\n",
      "\tCurrent Loss: 0.4214\n",
      "\tCurrent Loss: 0.4693\n",
      "\tCurrent Loss: 0.4041\n",
      "\tCurrent Loss: 0.4718\n",
      "\tCurrent Loss: 0.4028\n",
      "\tCurrent Loss: 0.4388\n",
      "\tCurrent Loss: 0.4392\n",
      "\tCurrent Loss: 0.4317\n",
      "\tCurrent Loss: 0.4561\n",
      "\tCurrent Loss: 0.4633\n",
      "Train Epoch 9 Train Loss: 0.4231575343132019 Valid Acc: 78.46%\n",
      "\tCurrent Loss: 0.3525\n",
      "\tCurrent Loss: 0.3363\n",
      "\tCurrent Loss: 0.3335\n",
      "\tCurrent Loss: 0.3514\n",
      "\tCurrent Loss: 0.3353\n",
      "\tCurrent Loss: 0.4090\n",
      "\tCurrent Loss: 0.3596\n",
      "\tCurrent Loss: 0.3363\n",
      "\tCurrent Loss: 0.3876\n",
      "\tCurrent Loss: 0.3597\n",
      "\tCurrent Loss: 0.3390\n",
      "\tCurrent Loss: 0.3868\n",
      "\tCurrent Loss: 0.3936\n",
      "\tCurrent Loss: 0.3856\n",
      "\tCurrent Loss: 0.4386\n",
      "\tCurrent Loss: 0.3745\n",
      "\tCurrent Loss: 0.3866\n",
      "\tCurrent Loss: 0.3656\n",
      "\tCurrent Loss: 0.3269\n",
      "\tCurrent Loss: 0.4125\n",
      "Train Epoch 10 Train Loss: 0.36869987864494325 Valid Acc: 80.58%\n",
      "\tCurrent Loss: 0.2789\n",
      "\tCurrent Loss: 0.2363\n",
      "\tCurrent Loss: 0.3016\n",
      "\tCurrent Loss: 0.2749\n",
      "\tCurrent Loss: 0.3068\n",
      "\tCurrent Loss: 0.2856\n",
      "\tCurrent Loss: 0.2951\n",
      "\tCurrent Loss: 0.3628\n",
      "\tCurrent Loss: 0.3217\n",
      "\tCurrent Loss: 0.3239\n",
      "\tCurrent Loss: 0.3349\n",
      "\tCurrent Loss: 0.3197\n",
      "\tCurrent Loss: 0.3486\n",
      "\tCurrent Loss: 0.3301\n",
      "\tCurrent Loss: 0.3356\n",
      "\tCurrent Loss: 0.3533\n",
      "\tCurrent Loss: 0.3166\n",
      "\tCurrent Loss: 0.3201\n",
      "\tCurrent Loss: 0.3149\n",
      "\tCurrent Loss: 0.3553\n",
      "Train Epoch 11 Train Loss: 0.31614846527576446 Valid Acc: 79.63%\n",
      "\tCurrent Loss: 0.2195\n",
      "\tCurrent Loss: 0.2612\n",
      "\tCurrent Loss: 0.2112\n",
      "\tCurrent Loss: 0.2127\n",
      "\tCurrent Loss: 0.2437\n",
      "\tCurrent Loss: 0.2394\n",
      "\tCurrent Loss: 0.2365\n",
      "\tCurrent Loss: 0.2551\n",
      "\tCurrent Loss: 0.2671\n",
      "\tCurrent Loss: 0.2980\n",
      "\tCurrent Loss: 0.3138\n",
      "\tCurrent Loss: 0.2968\n",
      "\tCurrent Loss: 0.2737\n",
      "\tCurrent Loss: 0.2888\n",
      "\tCurrent Loss: 0.3247\n",
      "\tCurrent Loss: 0.3502\n",
      "\tCurrent Loss: 0.2980\n",
      "\tCurrent Loss: 0.3217\n",
      "\tCurrent Loss: 0.3089\n",
      "\tCurrent Loss: 0.2506\n",
      "Train Epoch 12 Train Loss: 0.2731092157244682 Valid Acc: 79.62%\n",
      "\tCurrent Loss: 0.1918\n",
      "\tCurrent Loss: 0.1801\n",
      "\tCurrent Loss: 0.2044\n",
      "\tCurrent Loss: 0.1729\n",
      "\tCurrent Loss: 0.2157\n",
      "\tCurrent Loss: 0.2148\n",
      "\tCurrent Loss: 0.2119\n",
      "\tCurrent Loss: 0.2104\n",
      "\tCurrent Loss: 0.2160\n",
      "\tCurrent Loss: 0.2133\n",
      "\tCurrent Loss: 0.2140\n",
      "\tCurrent Loss: 0.2379\n",
      "\tCurrent Loss: 0.2676\n",
      "\tCurrent Loss: 0.2806\n",
      "\tCurrent Loss: 0.2428\n",
      "\tCurrent Loss: 0.2296\n",
      "\tCurrent Loss: 0.2510\n",
      "\tCurrent Loss: 0.2363\n",
      "\tCurrent Loss: 0.2401\n",
      "\tCurrent Loss: 0.2472\n",
      "Train Epoch 13 Train Loss: 0.22422570382356644 Valid Acc: 81.37%\n",
      "\tCurrent Loss: 0.1413\n",
      "\tCurrent Loss: 0.1514\n",
      "\tCurrent Loss: 0.1698\n",
      "\tCurrent Loss: 0.1445\n",
      "\tCurrent Loss: 0.1587\n",
      "\tCurrent Loss: 0.1626\n",
      "\tCurrent Loss: 0.1641\n",
      "\tCurrent Loss: 0.2033\n",
      "\tCurrent Loss: 0.2023\n",
      "\tCurrent Loss: 0.1882\n",
      "\tCurrent Loss: 0.1701\n",
      "\tCurrent Loss: 0.2147\n",
      "\tCurrent Loss: 0.2200\n",
      "\tCurrent Loss: 0.2119\n",
      "\tCurrent Loss: 0.1989\n",
      "\tCurrent Loss: 0.2168\n",
      "\tCurrent Loss: 0.2067\n",
      "\tCurrent Loss: 0.1888\n",
      "\tCurrent Loss: 0.2439\n",
      "\tCurrent Loss: 0.2153\n",
      "Train Epoch 14 Train Loss: 0.18915053936243056 Valid Acc: 80.39%\n",
      "\tCurrent Loss: 0.1152\n",
      "\tCurrent Loss: 0.1044\n",
      "\tCurrent Loss: 0.1178\n",
      "\tCurrent Loss: 0.1108\n",
      "\tCurrent Loss: 0.1279\n",
      "\tCurrent Loss: 0.1543\n",
      "\tCurrent Loss: 0.1256\n",
      "\tCurrent Loss: 0.1589\n",
      "\tCurrent Loss: 0.1916\n",
      "\tCurrent Loss: 0.1686\n",
      "\tCurrent Loss: 0.1451\n",
      "\tCurrent Loss: 0.1834\n",
      "\tCurrent Loss: 0.1954\n",
      "\tCurrent Loss: 0.1692\n",
      "\tCurrent Loss: 0.1779\n",
      "\tCurrent Loss: 0.1735\n",
      "\tCurrent Loss: 0.1876\n",
      "\tCurrent Loss: 0.2093\n",
      "\tCurrent Loss: 0.1830\n",
      "\tCurrent Loss: 0.1636\n",
      "Train Epoch 15 Train Loss: 0.15776725398898125 Valid Acc: 81.81%\n",
      "\tCurrent Loss: 0.1153\n",
      "\tCurrent Loss: 0.1148\n",
      "\tCurrent Loss: 0.1038\n",
      "\tCurrent Loss: 0.1044\n",
      "\tCurrent Loss: 0.0909\n",
      "\tCurrent Loss: 0.0887\n",
      "\tCurrent Loss: 0.1033\n",
      "\tCurrent Loss: 0.1241\n",
      "\tCurrent Loss: 0.1149\n",
      "\tCurrent Loss: 0.1128\n",
      "\tCurrent Loss: 0.1199\n",
      "\tCurrent Loss: 0.1457\n",
      "\tCurrent Loss: 0.1422\n",
      "\tCurrent Loss: 0.1496\n",
      "\tCurrent Loss: 0.1637\n",
      "\tCurrent Loss: 0.1524\n",
      "\tCurrent Loss: 0.1400\n",
      "\tCurrent Loss: 0.1864\n",
      "\tCurrent Loss: 0.1691\n",
      "\tCurrent Loss: 0.1423\n",
      "Train Epoch 16 Train Loss: 0.12903231717646121 Valid Acc: 80.94%\n",
      "\tCurrent Loss: 0.0830\n",
      "\tCurrent Loss: 0.0750\n",
      "\tCurrent Loss: 0.0717\n",
      "\tCurrent Loss: 0.0829\n",
      "\tCurrent Loss: 0.0938\n",
      "\tCurrent Loss: 0.0821\n",
      "\tCurrent Loss: 0.0935\n",
      "\tCurrent Loss: 0.1233\n",
      "\tCurrent Loss: 0.0827\n",
      "\tCurrent Loss: 0.0916\n",
      "\tCurrent Loss: 0.1189\n",
      "\tCurrent Loss: 0.1188\n",
      "\tCurrent Loss: 0.1090\n",
      "\tCurrent Loss: 0.1050\n",
      "\tCurrent Loss: 0.1069\n",
      "\tCurrent Loss: 0.1143\n",
      "\tCurrent Loss: 0.1299\n",
      "\tCurrent Loss: 0.1071\n",
      "\tCurrent Loss: 0.1078\n",
      "\tCurrent Loss: 0.1294\n",
      "Train Epoch 17 Train Loss: 0.10237102722972631 Valid Acc: 81.6%\n",
      "\tCurrent Loss: 0.0766\n",
      "\tCurrent Loss: 0.0839\n",
      "\tCurrent Loss: 0.0810\n",
      "\tCurrent Loss: 0.0684\n",
      "\tCurrent Loss: 0.0896\n",
      "\tCurrent Loss: 0.0916\n",
      "\tCurrent Loss: 0.0930\n",
      "\tCurrent Loss: 0.1219\n",
      "\tCurrent Loss: 0.0995\n",
      "\tCurrent Loss: 0.0733\n",
      "\tCurrent Loss: 0.1092\n",
      "\tCurrent Loss: 0.0956\n",
      "\tCurrent Loss: 0.0921\n",
      "\tCurrent Loss: 0.0749\n",
      "\tCurrent Loss: 0.0965\n",
      "\tCurrent Loss: 0.0963\n",
      "\tCurrent Loss: 0.1083\n",
      "\tCurrent Loss: 0.1165\n",
      "\tCurrent Loss: 0.1364\n",
      "\tCurrent Loss: 0.1232\n",
      "Train Epoch 18 Train Loss: 0.09725028576254845 Valid Acc: 81.32%\n",
      "\tCurrent Loss: 0.0843\n",
      "\tCurrent Loss: 0.0811\n",
      "\tCurrent Loss: 0.0695\n",
      "\tCurrent Loss: 0.0764\n",
      "\tCurrent Loss: 0.0817\n",
      "\tCurrent Loss: 0.0558\n",
      "\tCurrent Loss: 0.0633\n",
      "\tCurrent Loss: 0.0691\n",
      "\tCurrent Loss: 0.0901\n",
      "\tCurrent Loss: 0.0896\n",
      "\tCurrent Loss: 0.0743\n",
      "\tCurrent Loss: 0.0827\n",
      "\tCurrent Loss: 0.0886\n",
      "\tCurrent Loss: 0.0753\n",
      "\tCurrent Loss: 0.1055\n",
      "\tCurrent Loss: 0.1046\n",
      "\tCurrent Loss: 0.1372\n",
      "\tCurrent Loss: 0.1428\n",
      "\tCurrent Loss: 0.1358\n",
      "\tCurrent Loss: 0.1287\n",
      "Train Epoch 19 Train Loss: 0.09288193340152502 Valid Acc: 80.33%\n"
     ]
    }
   ],
   "source": [
    "best_valid_acc = -1\n",
    "optimizer = torch.optim.Adam(custom_resnet50.parameters())\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "   \n",
    "    train_loss = train(custom_resnet50, train_loader, optimizer, criterion)\n",
    "    valid_acc = evaluate(custom_resnet50, valid_loader)\n",
    "    \n",
    "    print('Train Epoch {} Train Loss: {} Valid Acc: {}%'.format(epoch, train_loss, valid_acc))\n",
    "    \n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        torch.save(custom_resnet50.state_dict(), 'custom_resnet50.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "_jjIfEm1Bp1O"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of total 10000 images: 81.52%\n"
     ]
    }
   ],
   "source": [
    "custom_resnet50.load_state_dict(torch.load('custom_resnet50.pth'))\n",
    "test(custom_resnet50, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet18(\n",
       "  (conv): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "  (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (maxpool2d): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layers): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (3): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (4): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (5): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (6): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (7): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (avgpool2d): AvgPool2d(kernel_size=7, stride=1, padding=0)\n",
       "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "custom_resnet18.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "W9eSXWepBp1Q"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tCurrent Loss: 2.2888\n",
      "\tCurrent Loss: 1.9298\n",
      "\tCurrent Loss: 1.8464\n",
      "\tCurrent Loss: 1.7634\n",
      "\tCurrent Loss: 1.7513\n",
      "\tCurrent Loss: 1.7119\n",
      "\tCurrent Loss: 1.6601\n",
      "\tCurrent Loss: 1.6776\n",
      "\tCurrent Loss: 1.6055\n",
      "\tCurrent Loss: 1.5759\n",
      "\tCurrent Loss: 1.5508\n",
      "\tCurrent Loss: 1.5211\n",
      "\tCurrent Loss: 1.4664\n",
      "\tCurrent Loss: 1.4488\n",
      "\tCurrent Loss: 1.4054\n",
      "\tCurrent Loss: 1.3424\n",
      "\tCurrent Loss: 1.2840\n",
      "\tCurrent Loss: 1.2775\n",
      "\tCurrent Loss: 1.2538\n",
      "\tCurrent Loss: 1.2935\n",
      "Train Epoch 0 Train Loss: 1.5696773218154907 Valid Acc: 46.71%\n",
      "\tCurrent Loss: 1.2148\n",
      "\tCurrent Loss: 1.2132\n",
      "\tCurrent Loss: 1.1896\n",
      "\tCurrent Loss: 1.1708\n",
      "\tCurrent Loss: 1.1561\n",
      "\tCurrent Loss: 1.1039\n",
      "\tCurrent Loss: 1.0835\n",
      "\tCurrent Loss: 1.0932\n",
      "\tCurrent Loss: 1.0961\n",
      "\tCurrent Loss: 1.0647\n",
      "\tCurrent Loss: 1.0473\n",
      "\tCurrent Loss: 1.0392\n",
      "\tCurrent Loss: 1.0062\n",
      "\tCurrent Loss: 0.9624\n",
      "\tCurrent Loss: 0.9906\n",
      "\tCurrent Loss: 1.0160\n",
      "\tCurrent Loss: 0.9327\n",
      "\tCurrent Loss: 0.9100\n",
      "\tCurrent Loss: 0.9049\n",
      "\tCurrent Loss: 0.9188\n",
      "Train Epoch 1 Train Loss: 1.049575322818756 Valid Acc: 64.54%\n",
      "\tCurrent Loss: 0.8167\n",
      "\tCurrent Loss: 0.8393\n",
      "\tCurrent Loss: 0.8391\n",
      "\tCurrent Loss: 0.8043\n",
      "\tCurrent Loss: 0.8308\n",
      "\tCurrent Loss: 0.8253\n",
      "\tCurrent Loss: 0.8390\n",
      "\tCurrent Loss: 0.8141\n",
      "\tCurrent Loss: 0.7890\n",
      "\tCurrent Loss: 0.8037\n",
      "\tCurrent Loss: 0.7837\n",
      "\tCurrent Loss: 0.8026\n",
      "\tCurrent Loss: 0.7845\n",
      "\tCurrent Loss: 0.7719\n",
      "\tCurrent Loss: 0.7439\n",
      "\tCurrent Loss: 0.7621\n",
      "\tCurrent Loss: 0.7452\n",
      "\tCurrent Loss: 0.7600\n",
      "\tCurrent Loss: 0.7484\n",
      "\tCurrent Loss: 0.6807\n",
      "Train Epoch 2 Train Loss: 0.7864421087741852 Valid Acc: 70.41%\n",
      "\tCurrent Loss: 0.6262\n",
      "\tCurrent Loss: 0.6313\n",
      "\tCurrent Loss: 0.6128\n",
      "\tCurrent Loss: 0.6712\n",
      "\tCurrent Loss: 0.6813\n",
      "\tCurrent Loss: 0.5729\n",
      "\tCurrent Loss: 0.6757\n",
      "\tCurrent Loss: 0.6069\n",
      "\tCurrent Loss: 0.5951\n",
      "\tCurrent Loss: 0.6599\n",
      "\tCurrent Loss: 0.6000\n",
      "\tCurrent Loss: 0.6193\n",
      "\tCurrent Loss: 0.6372\n",
      "\tCurrent Loss: 0.5927\n",
      "\tCurrent Loss: 0.5824\n",
      "\tCurrent Loss: 0.6189\n",
      "\tCurrent Loss: 0.6178\n",
      "\tCurrent Loss: 0.5779\n",
      "\tCurrent Loss: 0.6413\n",
      "\tCurrent Loss: 0.6249\n",
      "Train Epoch 3 Train Loss: 0.6210933429718017 Valid Acc: 77.39%\n",
      "\tCurrent Loss: 0.4800\n",
      "\tCurrent Loss: 0.4603\n",
      "\tCurrent Loss: 0.5127\n",
      "\tCurrent Loss: 0.5196\n",
      "\tCurrent Loss: 0.5506\n",
      "\tCurrent Loss: 0.5185\n",
      "\tCurrent Loss: 0.5292\n",
      "\tCurrent Loss: 0.5079\n",
      "\tCurrent Loss: 0.4693\n",
      "\tCurrent Loss: 0.4770\n",
      "\tCurrent Loss: 0.4722\n",
      "\tCurrent Loss: 0.4842\n",
      "\tCurrent Loss: 0.4990\n",
      "\tCurrent Loss: 0.4750\n",
      "\tCurrent Loss: 0.5361\n",
      "\tCurrent Loss: 0.5124\n",
      "\tCurrent Loss: 0.4528\n",
      "\tCurrent Loss: 0.5013\n",
      "\tCurrent Loss: 0.5373\n",
      "\tCurrent Loss: 0.5019\n",
      "Train Epoch 4 Train Loss: 0.5023854075670242 Valid Acc: 74.93%\n",
      "\tCurrent Loss: 0.4121\n",
      "\tCurrent Loss: 0.3910\n",
      "\tCurrent Loss: 0.3817\n",
      "\tCurrent Loss: 0.3603\n",
      "\tCurrent Loss: 0.4062\n",
      "\tCurrent Loss: 0.4070\n",
      "\tCurrent Loss: 0.4136\n",
      "\tCurrent Loss: 0.4089\n",
      "\tCurrent Loss: 0.4354\n",
      "\tCurrent Loss: 0.4157\n",
      "\tCurrent Loss: 0.4139\n",
      "\tCurrent Loss: 0.3981\n",
      "\tCurrent Loss: 0.3910\n",
      "\tCurrent Loss: 0.4339\n",
      "\tCurrent Loss: 0.4390\n",
      "\tCurrent Loss: 0.4432\n",
      "\tCurrent Loss: 0.4222\n",
      "\tCurrent Loss: 0.4094\n",
      "\tCurrent Loss: 0.3740\n",
      "\tCurrent Loss: 0.4005\n",
      "Train Epoch 5 Train Loss: 0.40639288873672486 Valid Acc: 77.53%\n",
      "\tCurrent Loss: 0.2888\n",
      "\tCurrent Loss: 0.2983\n",
      "\tCurrent Loss: 0.2951\n",
      "\tCurrent Loss: 0.2973\n",
      "\tCurrent Loss: 0.3220\n",
      "\tCurrent Loss: 0.3092\n",
      "\tCurrent Loss: 0.3418\n",
      "\tCurrent Loss: 0.3185\n",
      "\tCurrent Loss: 0.3108\n",
      "\tCurrent Loss: 0.3489\n",
      "\tCurrent Loss: 0.3701\n",
      "\tCurrent Loss: 0.2992\n",
      "\tCurrent Loss: 0.3290\n",
      "\tCurrent Loss: 0.3370\n",
      "\tCurrent Loss: 0.3283\n",
      "\tCurrent Loss: 0.3282\n",
      "\tCurrent Loss: 0.3153\n",
      "\tCurrent Loss: 0.3329\n",
      "\tCurrent Loss: 0.3397\n",
      "\tCurrent Loss: 0.2977\n",
      "Train Epoch 6 Train Loss: 0.3217351233124733 Valid Acc: 78.99%\n",
      "\tCurrent Loss: 0.2361\n",
      "\tCurrent Loss: 0.2077\n",
      "\tCurrent Loss: 0.1929\n",
      "\tCurrent Loss: 0.1900\n",
      "\tCurrent Loss: 0.1980\n",
      "\tCurrent Loss: 0.2086\n",
      "\tCurrent Loss: 0.2369\n",
      "\tCurrent Loss: 0.2358\n",
      "\tCurrent Loss: 0.2529\n",
      "\tCurrent Loss: 0.2576\n",
      "\tCurrent Loss: 0.2480\n",
      "\tCurrent Loss: 0.2542\n",
      "\tCurrent Loss: 0.2675\n",
      "\tCurrent Loss: 0.2575\n",
      "\tCurrent Loss: 0.2609\n",
      "\tCurrent Loss: 0.2503\n",
      "\tCurrent Loss: 0.2199\n",
      "\tCurrent Loss: 0.2671\n",
      "\tCurrent Loss: 0.2459\n",
      "\tCurrent Loss: 0.2871\n",
      "Train Epoch 7 Train Loss: 0.241042138171196 Valid Acc: 79.87%\n",
      "\tCurrent Loss: 0.1776\n",
      "\tCurrent Loss: 0.1158\n",
      "\tCurrent Loss: 0.1455\n",
      "\tCurrent Loss: 0.1237\n",
      "\tCurrent Loss: 0.1360\n",
      "\tCurrent Loss: 0.1506\n",
      "\tCurrent Loss: 0.1317\n",
      "\tCurrent Loss: 0.1894\n",
      "\tCurrent Loss: 0.1742\n",
      "\tCurrent Loss: 0.1848\n",
      "\tCurrent Loss: 0.1760\n",
      "\tCurrent Loss: 0.2131\n",
      "\tCurrent Loss: 0.2135\n",
      "\tCurrent Loss: 0.1569\n",
      "\tCurrent Loss: 0.1923\n",
      "\tCurrent Loss: 0.2185\n",
      "\tCurrent Loss: 0.1844\n",
      "\tCurrent Loss: 0.1963\n",
      "\tCurrent Loss: 0.1917\n",
      "\tCurrent Loss: 0.2088\n",
      "Train Epoch 8 Train Loss: 0.17505755176842214 Valid Acc: 81.43%\n",
      "\tCurrent Loss: 0.1179\n",
      "\tCurrent Loss: 0.1065\n",
      "\tCurrent Loss: 0.0998\n",
      "\tCurrent Loss: 0.1026\n",
      "\tCurrent Loss: 0.0862\n",
      "\tCurrent Loss: 0.1145\n",
      "\tCurrent Loss: 0.1218\n",
      "\tCurrent Loss: 0.0923\n",
      "\tCurrent Loss: 0.1004\n",
      "\tCurrent Loss: 0.1242\n",
      "\tCurrent Loss: 0.1409\n",
      "\tCurrent Loss: 0.1228\n",
      "\tCurrent Loss: 0.1417\n",
      "\tCurrent Loss: 0.1321\n",
      "\tCurrent Loss: 0.1191\n",
      "\tCurrent Loss: 0.1582\n",
      "\tCurrent Loss: 0.1457\n",
      "\tCurrent Loss: 0.1391\n",
      "\tCurrent Loss: 0.1548\n",
      "\tCurrent Loss: 0.1360\n",
      "Train Epoch 9 Train Loss: 0.12274741463661194 Valid Acc: 81.38%\n",
      "\tCurrent Loss: 0.0777\n",
      "\tCurrent Loss: 0.0642\n",
      "\tCurrent Loss: 0.0789\n",
      "\tCurrent Loss: 0.0769\n",
      "\tCurrent Loss: 0.0840\n",
      "\tCurrent Loss: 0.0779\n",
      "\tCurrent Loss: 0.1008\n",
      "\tCurrent Loss: 0.0749\n",
      "\tCurrent Loss: 0.1111\n",
      "\tCurrent Loss: 0.0891\n",
      "\tCurrent Loss: 0.0944\n",
      "\tCurrent Loss: 0.1116\n",
      "\tCurrent Loss: 0.1054\n",
      "\tCurrent Loss: 0.1084\n",
      "\tCurrent Loss: 0.1093\n",
      "\tCurrent Loss: 0.1554\n",
      "\tCurrent Loss: 0.1210\n",
      "\tCurrent Loss: 0.1102\n",
      "\tCurrent Loss: 0.1206\n",
      "\tCurrent Loss: 0.1151\n",
      "Train Epoch 10 Train Loss: 0.10045071358382703 Valid Acc: 81.3%\n",
      "\tCurrent Loss: 0.0655\n",
      "\tCurrent Loss: 0.0691\n",
      "\tCurrent Loss: 0.0444\n",
      "\tCurrent Loss: 0.0595\n",
      "\tCurrent Loss: 0.0480\n",
      "\tCurrent Loss: 0.0525\n",
      "\tCurrent Loss: 0.0527\n",
      "\tCurrent Loss: 0.0544\n",
      "\tCurrent Loss: 0.0669\n",
      "\tCurrent Loss: 0.0811\n",
      "\tCurrent Loss: 0.0647\n",
      "\tCurrent Loss: 0.0900\n",
      "\tCurrent Loss: 0.0982\n",
      "\tCurrent Loss: 0.1023\n",
      "\tCurrent Loss: 0.1176\n",
      "\tCurrent Loss: 0.0871\n",
      "\tCurrent Loss: 0.0876\n",
      "\tCurrent Loss: 0.0789\n",
      "\tCurrent Loss: 0.0872\n",
      "\tCurrent Loss: 0.0825\n",
      "Train Epoch 11 Train Loss: 0.07533265860676766 Valid Acc: 82.09%\n",
      "\tCurrent Loss: 0.0502\n",
      "\tCurrent Loss: 0.0488\n",
      "\tCurrent Loss: 0.0359\n",
      "\tCurrent Loss: 0.0389\n",
      "\tCurrent Loss: 0.0345\n",
      "\tCurrent Loss: 0.0569\n",
      "\tCurrent Loss: 0.0521\n",
      "\tCurrent Loss: 0.0558\n",
      "\tCurrent Loss: 0.0703\n",
      "\tCurrent Loss: 0.0577\n",
      "\tCurrent Loss: 0.0785\n",
      "\tCurrent Loss: 0.0783\n",
      "\tCurrent Loss: 0.1006\n",
      "\tCurrent Loss: 0.0858\n",
      "\tCurrent Loss: 0.0912\n",
      "\tCurrent Loss: 0.0984\n",
      "\tCurrent Loss: 0.0868\n",
      "\tCurrent Loss: 0.0952\n",
      "\tCurrent Loss: 0.0869\n",
      "\tCurrent Loss: 0.1024\n",
      "Train Epoch 12 Train Loss: 0.07146553736180067 Valid Acc: 80.78%\n",
      "\tCurrent Loss: 0.0594\n",
      "\tCurrent Loss: 0.0444\n",
      "\tCurrent Loss: 0.0526\n",
      "\tCurrent Loss: 0.0635\n",
      "\tCurrent Loss: 0.0545\n",
      "\tCurrent Loss: 0.0770\n",
      "\tCurrent Loss: 0.0738\n",
      "\tCurrent Loss: 0.0659\n",
      "\tCurrent Loss: 0.0733\n",
      "\tCurrent Loss: 0.0618\n",
      "\tCurrent Loss: 0.0500\n",
      "\tCurrent Loss: 0.0787\n",
      "\tCurrent Loss: 0.0696\n",
      "\tCurrent Loss: 0.0616\n",
      "\tCurrent Loss: 0.0818\n",
      "\tCurrent Loss: 0.1057\n",
      "\tCurrent Loss: 0.0922\n",
      "\tCurrent Loss: 0.1022\n",
      "\tCurrent Loss: 0.1049\n",
      "\tCurrent Loss: 0.0871\n",
      "Train Epoch 13 Train Loss: 0.0737036158412695 Valid Acc: 78.57%\n",
      "\tCurrent Loss: 0.0480\n",
      "\tCurrent Loss: 0.0381\n",
      "\tCurrent Loss: 0.0263\n",
      "\tCurrent Loss: 0.0250\n",
      "\tCurrent Loss: 0.0318\n",
      "\tCurrent Loss: 0.0304\n",
      "\tCurrent Loss: 0.0341\n",
      "\tCurrent Loss: 0.0373\n",
      "\tCurrent Loss: 0.0443\n",
      "\tCurrent Loss: 0.0369\n",
      "\tCurrent Loss: 0.0436\n",
      "\tCurrent Loss: 0.0369\n",
      "\tCurrent Loss: 0.0282\n",
      "\tCurrent Loss: 0.0369\n",
      "\tCurrent Loss: 0.0472\n",
      "\tCurrent Loss: 0.0558\n",
      "\tCurrent Loss: 0.0602\n",
      "\tCurrent Loss: 0.0703\n",
      "\tCurrent Loss: 0.0573\n",
      "\tCurrent Loss: 0.0675\n",
      "Train Epoch 14 Train Loss: 0.04347925008758902 Valid Acc: 81.97%\n",
      "\tCurrent Loss: 0.0440\n",
      "\tCurrent Loss: 0.0395\n",
      "\tCurrent Loss: 0.0437\n",
      "\tCurrent Loss: 0.0314\n",
      "\tCurrent Loss: 0.0443\n",
      "\tCurrent Loss: 0.0422\n",
      "\tCurrent Loss: 0.0426\n",
      "\tCurrent Loss: 0.0358\n",
      "\tCurrent Loss: 0.0389\n",
      "\tCurrent Loss: 0.0482\n",
      "\tCurrent Loss: 0.0525\n",
      "\tCurrent Loss: 0.0565\n",
      "\tCurrent Loss: 0.0483\n",
      "\tCurrent Loss: 0.0521\n",
      "\tCurrent Loss: 0.0645\n",
      "\tCurrent Loss: 0.0603\n",
      "\tCurrent Loss: 0.0629\n",
      "\tCurrent Loss: 0.0718\n",
      "\tCurrent Loss: 0.0719\n",
      "\tCurrent Loss: 0.0652\n",
      "Train Epoch 15 Train Loss: 0.05206567675694823 Valid Acc: 79.39%\n",
      "\tCurrent Loss: 0.0712\n",
      "\tCurrent Loss: 0.0510\n",
      "\tCurrent Loss: 0.0565\n",
      "\tCurrent Loss: 0.0432\n",
      "\tCurrent Loss: 0.0457\n",
      "\tCurrent Loss: 0.0592\n",
      "\tCurrent Loss: 0.0431\n",
      "\tCurrent Loss: 0.0283\n",
      "\tCurrent Loss: 0.0453\n",
      "\tCurrent Loss: 0.0528\n",
      "\tCurrent Loss: 0.0570\n",
      "\tCurrent Loss: 0.0651\n",
      "\tCurrent Loss: 0.0524\n",
      "\tCurrent Loss: 0.0515\n",
      "\tCurrent Loss: 0.0678\n",
      "\tCurrent Loss: 0.0694\n",
      "\tCurrent Loss: 0.0634\n",
      "\tCurrent Loss: 0.0574\n",
      "\tCurrent Loss: 0.0567\n",
      "\tCurrent Loss: 0.0612\n",
      "Train Epoch 16 Train Loss: 0.05545057677477598 Valid Acc: 79.96%\n",
      "\tCurrent Loss: 0.0409\n",
      "\tCurrent Loss: 0.0316\n",
      "\tCurrent Loss: 0.0296\n",
      "\tCurrent Loss: 0.0248\n",
      "\tCurrent Loss: 0.0295\n",
      "\tCurrent Loss: 0.0188\n",
      "\tCurrent Loss: 0.0246\n",
      "\tCurrent Loss: 0.0242\n",
      "\tCurrent Loss: 0.0316\n",
      "\tCurrent Loss: 0.0414\n",
      "\tCurrent Loss: 0.0491\n",
      "\tCurrent Loss: 0.0389\n",
      "\tCurrent Loss: 0.0454\n",
      "\tCurrent Loss: 0.0414\n",
      "\tCurrent Loss: 0.0402\n",
      "\tCurrent Loss: 0.0595\n",
      "\tCurrent Loss: 0.0442\n",
      "\tCurrent Loss: 0.0406\n",
      "\tCurrent Loss: 0.0408\n",
      "\tCurrent Loss: 0.0538\n",
      "Train Epoch 17 Train Loss: 0.03793879156410694 Valid Acc: 81.55%\n",
      "\tCurrent Loss: 0.0323\n",
      "\tCurrent Loss: 0.0380\n",
      "\tCurrent Loss: 0.0391\n",
      "\tCurrent Loss: 0.0363\n",
      "\tCurrent Loss: 0.0428\n",
      "\tCurrent Loss: 0.0460\n",
      "\tCurrent Loss: 0.0272\n",
      "\tCurrent Loss: 0.0299\n",
      "\tCurrent Loss: 0.0236\n",
      "\tCurrent Loss: 0.0259\n",
      "\tCurrent Loss: 0.0392\n",
      "\tCurrent Loss: 0.0390\n",
      "\tCurrent Loss: 0.0527\n",
      "\tCurrent Loss: 0.0546\n",
      "\tCurrent Loss: 0.0504\n",
      "\tCurrent Loss: 0.0373\n",
      "\tCurrent Loss: 0.0248\n",
      "\tCurrent Loss: 0.0413\n",
      "\tCurrent Loss: 0.0358\n",
      "\tCurrent Loss: 0.0428\n",
      "Train Epoch 18 Train Loss: 0.03840213737413287 Valid Acc: 82.54%\n",
      "\tCurrent Loss: 0.0314\n",
      "\tCurrent Loss: 0.0415\n",
      "\tCurrent Loss: 0.0341\n",
      "\tCurrent Loss: 0.0359\n",
      "\tCurrent Loss: 0.0356\n",
      "\tCurrent Loss: 0.0435\n",
      "\tCurrent Loss: 0.0372\n",
      "\tCurrent Loss: 0.0340\n",
      "\tCurrent Loss: 0.0259\n",
      "\tCurrent Loss: 0.0290\n",
      "\tCurrent Loss: 0.0191\n",
      "\tCurrent Loss: 0.0322\n",
      "\tCurrent Loss: 0.0454\n",
      "\tCurrent Loss: 0.0450\n",
      "\tCurrent Loss: 0.0421\n",
      "\tCurrent Loss: 0.0328\n",
      "\tCurrent Loss: 0.0308\n",
      "\tCurrent Loss: 0.0245\n",
      "\tCurrent Loss: 0.0321\n",
      "\tCurrent Loss: 0.0313\n",
      "Train Epoch 19 Train Loss: 0.034770199802517894 Valid Acc: 81.99%\n"
     ]
    }
   ],
   "source": [
    "best_valid_acc = -1\n",
    "optimizer = torch.optim.Adam(custom_resnet18.parameters())\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "   \n",
    "    train_loss = train(custom_resnet18, train_loader, optimizer, criterion)\n",
    "    valid_acc = evaluate(custom_resnet18, valid_loader)\n",
    "    \n",
    "    print('Train Epoch {} Train Loss: {} Valid Acc: {}%'.format(epoch, train_loss, valid_acc))\n",
    "    \n",
    "    if valid_acc > best_valid_acc:\n",
    "        best_valid_acc = valid_acc\n",
    "        torch.save(custom_resnet18.state_dict(), 'custom_resnet18.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "efiveGdSBp1S"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of total 10000 images: 82.51%\n"
     ]
    }
   ],
   "source": [
    "custom_resnet18.load_state_dict(torch.load('custom_resnet18.pth'))\n",
    "test(custom_resnet18, test_loader)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "notebookfcfc6bce13.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
